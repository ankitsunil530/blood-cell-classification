{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sU49HUdckIID",
        "outputId": "a1f8c525-3227-4ee0-b1db-2bc9e7c1332a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "fatal: destination path 'blood-cell-classification' already exists and is not an empty directory.\n",
            "/content/blood-cell-classification/blood-cell-classification\n"
          ]
        }
      ],
      "source": [
        "# !git clone https://github.com/ankitsunil530/blood-cell-classification.git\n",
        "# %cd blood-cell-classification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aiGH1t3MkQ8D",
        "outputId": "3119b7e3-ce71-43d5-fc1f-d12300e470d6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Blood Cell Classification**"
      ],
      "metadata": {
        "id": "o77FxEgYMI7U"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c1bd6044"
      },
      "source": [
        "## Data loading and preprocessing\n",
        "\n",
        "### Subtask:\n",
        "Load the blood cell dataset and preprocess the images for use with the vision transformer and Performer models. This will involve resizing, normalization, and splitting the data into training, validation, and test sets.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "e819139e"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "caabfb6a"
      },
      "outputs": [],
      "source": [
        "DATASET_PATH = \"/content/drive/MyDrive/Deep Learning Lab/Blood_Cells_Dataset/Blood_Cells_Dataset\"\n",
        "IMG_HEIGHT = 224\n",
        "IMG_WIDTH = 224\n",
        "\n",
        "def load_images_and_labels(dataset_path):\n",
        "    images = []\n",
        "    labels = []\n",
        "    class_names = sorted(os.listdir(dataset_path))\n",
        "    class_to_idx = {class_name: i for i, class_name in enumerate(class_names)}\n",
        "\n",
        "    for class_name in class_names:\n",
        "        class_dir = os.path.join(dataset_path, class_name)\n",
        "        if os.path.isdir(class_dir):\n",
        "            for img_name in os.listdir(class_dir):\n",
        "                img_path = os.path.join(class_dir, img_name)\n",
        "                img = cv2.imread(img_path)\n",
        "                if img is not None:\n",
        "                    img = cv2.resize(img, (IMG_WIDTH, IMG_HEIGHT))\n",
        "                    images.append(img)\n",
        "                    labels.append(class_to_idx[class_name])\n",
        "\n",
        "    return np.array(images), np.array(labels), class_names\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5v91fwvHf3YN",
        "outputId": "50211248-6f50-496e-ad79-b335218cfc7a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Blood_Cells_Dataset/\n",
            "    dataset2-master/\n",
            "        dataset2-master/\n",
            "            images/\n",
            "                TEST_SIMPLE/\n",
            "                    MONOCYTE/\n",
            "                    NEUTROPHIL/\n",
            "                    LYMPHOCYTE/\n",
            "                    EOSINOPHIL/\n",
            "                TEST/\n",
            "                    NEUTROPHIL/\n",
            "                    MONOCYTE/\n",
            "                    LYMPHOCYTE/\n",
            "                    EOSINOPHIL/\n",
            "                TRAIN/\n",
            "                    NEUTROPHIL/\n",
            "                    MONOCYTE/\n",
            "                    LYMPHOCYTE/\n",
            "                    EOSINOPHIL/\n",
            "    dataset-master/\n",
            "        dataset-master/\n",
            "            Annotations/\n",
            "            JPEGImages/\n"
          ]
        }
      ],
      "source": [
        "for root, dirs, files in os.walk(DATASET_PATH):\n",
        "    level = root.replace(DATASET_PATH, '').count(os.sep)\n",
        "    indent = ' ' * 4 * (level)\n",
        "    print(f'{indent}{os.path.basename(root)}/')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c0b77a45",
        "outputId": "3107e169-b8e7-4831-c9dd-07ec7a07276f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found classes: ['EOSINOPHIL', 'LYMPHOCYTE', 'MONOCYTE', 'NEUTROPHIL']\n",
            "Processing directory: /content/drive/MyDrive/Deep Learning Lab/Blood_Cells_Dataset/Blood_Cells_Dataset/dataset2-master/dataset2-master/images/TRAIN/NEUTROPHIL\n",
            "Processing directory: /content/drive/MyDrive/Deep Learning Lab/Blood_Cells_Dataset/Blood_Cells_Dataset/dataset2-master/dataset2-master/images/TRAIN/MONOCYTE\n",
            "Processing directory: /content/drive/MyDrive/Deep Learning Lab/Blood_Cells_Dataset/Blood_Cells_Dataset/dataset2-master/dataset2-master/images/TRAIN/LYMPHOCYTE\n",
            "Processing directory: /content/drive/MyDrive/Deep Learning Lab/Blood_Cells_Dataset/Blood_Cells_Dataset/dataset2-master/dataset2-master/images/TRAIN/EOSINOPHIL\n",
            "Processing directory: /content/drive/MyDrive/Deep Learning Lab/Blood_Cells_Dataset/Blood_Cells_Dataset/dataset2-master/dataset2-master/images/TEST/NEUTROPHIL\n",
            "Processing directory: /content/drive/MyDrive/Deep Learning Lab/Blood_Cells_Dataset/Blood_Cells_Dataset/dataset2-master/dataset2-master/images/TEST/MONOCYTE\n",
            "Processing directory: /content/drive/MyDrive/Deep Learning Lab/Blood_Cells_Dataset/Blood_Cells_Dataset/dataset2-master/dataset2-master/images/TEST/LYMPHOCYTE\n",
            "Processing directory: /content/drive/MyDrive/Deep Learning Lab/Blood_Cells_Dataset/Blood_Cells_Dataset/dataset2-master/dataset2-master/images/TEST/EOSINOPHIL\n",
            "Processing directory: /content/drive/MyDrive/Deep Learning Lab/Blood_Cells_Dataset/Blood_Cells_Dataset/dataset2-master/dataset2-master/images/TEST_SIMPLE/MONOCYTE\n",
            "Processing directory: /content/drive/MyDrive/Deep Learning Lab/Blood_Cells_Dataset/Blood_Cells_Dataset/dataset2-master/dataset2-master/images/TEST_SIMPLE/NEUTROPHIL\n",
            "Processing directory: /content/drive/MyDrive/Deep Learning Lab/Blood_Cells_Dataset/Blood_Cells_Dataset/dataset2-master/dataset2-master/images/TEST_SIMPLE/LYMPHOCYTE\n",
            "Processing directory: /content/drive/MyDrive/Deep Learning Lab/Blood_Cells_Dataset/Blood_Cells_Dataset/dataset2-master/dataset2-master/images/TEST_SIMPLE/EOSINOPHIL\n",
            "Loaded 12515 images and 12515 labels.\n",
            "Training set size: 10012\n",
            "Validation set size: 1251\n",
            "Test set size: 1252\n",
            "Class names: ['EOSINOPHIL', 'LYMPHOCYTE', 'MONOCYTE', 'NEUTROPHIL']\n"
          ]
        }
      ],
      "source": [
        "def load_images_and_labels(dataset_path):\n",
        "    images = []\n",
        "    labels = []\n",
        "    # Update base directories to reflect the actual structure\n",
        "    base_image_dirs = [\n",
        "        os.path.join('dataset2-master', 'dataset2-master', 'images', 'TRAIN'),\n",
        "        os.path.join('dataset2-master', 'dataset2-master', 'images', 'TEST'),\n",
        "        os.path.join('dataset2-master', 'dataset2-master', 'images', 'TEST_SIMPLE')\n",
        "    ]\n",
        "    class_names = set()\n",
        "\n",
        "    # First pass to collect all unique class names\n",
        "    for base_img_dir_name in base_image_dirs:\n",
        "        base_img_dir_path = os.path.join(dataset_path, base_img_dir_name)\n",
        "        if os.path.isdir(base_img_dir_path):\n",
        "            for class_name in os.listdir(base_img_dir_path):\n",
        "                class_dir_path = os.path.join(base_img_dir_path, class_name)\n",
        "                if os.path.isdir(class_dir_path):\n",
        "                    class_names.add(class_name)\n",
        "\n",
        "    class_names = sorted(list(class_names))\n",
        "    class_to_idx = {class_name: i for i, class_name in enumerate(class_names)}\n",
        "\n",
        "    print(f\"Found classes: {class_names}\")\n",
        "\n",
        "    # Second pass to load images\n",
        "    for base_img_dir_name in base_image_dirs:\n",
        "        base_img_dir_path = os.path.join(dataset_path, base_img_dir_name)\n",
        "        if os.path.isdir(base_img_dir_path):\n",
        "            for class_name in os.listdir(base_img_dir_path):\n",
        "                class_dir_path = os.path.join(base_img_dir_path, class_name)\n",
        "                print(f\"Processing directory: {class_dir_path}\")\n",
        "                if os.path.isdir(class_dir_path) and class_name in class_to_idx:\n",
        "                    for img_name in os.listdir(class_dir_path):\n",
        "                        img_path = os.path.join(class_dir_path, img_name)\n",
        "                        if os.path.isfile(img_path) and img_name.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
        "                            img = cv2.imread(img_path)\n",
        "                            if img is not None:\n",
        "                                img = cv2.resize(img, (IMG_WIDTH, IMG_HEIGHT))\n",
        "                                images.append(img)\n",
        "                                labels.append(class_to_idx[class_name])\n",
        "                            else:\n",
        "                                print(f\"Could not load image: {img_path}\")\n",
        "\n",
        "    print(f\"Loaded {len(images)} images and {len(labels)} labels.\")\n",
        "    return np.array(images), np.array(labels), class_names\n",
        "\n",
        "images, labels, class_names = load_images_and_labels(DATASET_PATH)\n",
        "\n",
        "# Split the data - combining images from TRAIN, TEST, TEST_SIMPLE before splitting\n",
        "# Using a fixed random state for reproducibility\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(images, labels, test_size=0.2, stratify=labels, random_state=42)\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, stratify=y_temp, random_state=42)\n",
        "\n",
        "print(f\"Training set size: {len(X_train)}\")\n",
        "print(f\"Validation set size: {len(X_val)}\")\n",
        "print(f\"Test set size: {len(X_test)}\")\n",
        "print(f\"Class names: {class_names}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d77d6a6a"
      },
      "source": [
        "## Data loading and preprocessing\n",
        "\n",
        "### Subtask:\n",
        "Load the blood cell dataset and preprocess the images for use with the vision transformer and Performer models. This will involve resizing, normalization, and splitting the data into training, validation, and test sets.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "96d53a02"
      },
      "outputs": [],
      "source": [
        "class BloodCellDataset(Dataset):\n",
        "    def __init__(self, images, labels, transform=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            images (numpy array): Array of image data (H, W, C).\n",
        "            labels (numpy array): Array of corresponding labels.\n",
        "            transform (callable, optional): Optional transform to be applied\n",
        "                on a sample.\n",
        "        \"\"\"\n",
        "        self.images = images\n",
        "        self.labels = labels\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if torch.is_tensor(idx):\n",
        "            idx = idx.tolist()\n",
        "\n",
        "        image = self.images[idx]\n",
        "        label = self.labels[idx]\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, label\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "25837724",
        "outputId": "3f5858ea-30d4-49e7-b7f4-346399aceee2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of training batches: 313\n",
            "Number of validation batches: 40\n",
            "Number of test batches: 40\n"
          ]
        }
      ],
      "source": [
        "# Define transformations\n",
        "# Using common mean and std dev for ImageNet as a starting point\n",
        "# A more accurate approach would be to calculate these from the dataset\n",
        "mean = [0.485, 0.456, 0.406]\n",
        "std = [0.229, 0.224, 0.225]\n",
        "\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.ToPILImage(), # Convert numpy array to PIL Image for transforms\n",
        "    transforms.RandomResizedCrop(IMG_HEIGHT),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=mean, std=std),\n",
        "])\n",
        "\n",
        "val_test_transform = transforms.Compose([\n",
        "    transforms.ToPILImage(), # Convert numpy array to PIL Image for transforms\n",
        "    transforms.Resize(IMG_HEIGHT),\n",
        "    transforms.CenterCrop(IMG_HEIGHT),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=mean, std=std),\n",
        "])\n",
        "\n",
        "# Create Dataset instances\n",
        "train_dataset = BloodCellDataset(X_train, y_train, transform=train_transform)\n",
        "val_dataset = BloodCellDataset(X_val, y_val, transform=val_test_transform)\n",
        "test_dataset = BloodCellDataset(X_test, y_test, transform=val_test_transform)\n",
        "\n",
        "# Create DataLoader instances\n",
        "batch_size = 32 # Using the already defined batch_size\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "print(f\"Number of training batches: {len(train_dataloader)}\")\n",
        "print(f\"Number of validation batches: {len(val_dataloader)}\")\n",
        "print(f\"Number of test batches: {len(test_dataloader)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7757e7a6"
      },
      "source": [
        "## Vision transformer model implementation\n",
        "\n",
        "### Subtask:\n",
        "Implement the Vision Transformer model for blood cell classification.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "59d19bcc"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "from torchvision.models import vit_b_16, ViT_B_16_Weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7cae6304",
        "outputId": "9b62c8e1-8c62-4646-ce42-d544cfe90012"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model initialized and moved to cuda\n",
            "BloodCellViT(\n",
            "  (vit): VisionTransformer(\n",
            "    (conv_proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
            "    (encoder): Encoder(\n",
            "      (dropout): Dropout(p=0.0, inplace=False)\n",
            "      (layers): Sequential(\n",
            "        (encoder_layer_0): EncoderBlock(\n",
            "          (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "          (self_attention): MultiheadAttention(\n",
            "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
            "          )\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "          (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "          (mlp): MLPBlock(\n",
            "            (0): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (1): GELU(approximate='none')\n",
            "            (2): Dropout(p=0.0, inplace=False)\n",
            "            (3): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (4): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (encoder_layer_1): EncoderBlock(\n",
            "          (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "          (self_attention): MultiheadAttention(\n",
            "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
            "          )\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "          (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "          (mlp): MLPBlock(\n",
            "            (0): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (1): GELU(approximate='none')\n",
            "            (2): Dropout(p=0.0, inplace=False)\n",
            "            (3): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (4): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (encoder_layer_2): EncoderBlock(\n",
            "          (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "          (self_attention): MultiheadAttention(\n",
            "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
            "          )\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "          (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "          (mlp): MLPBlock(\n",
            "            (0): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (1): GELU(approximate='none')\n",
            "            (2): Dropout(p=0.0, inplace=False)\n",
            "            (3): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (4): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (encoder_layer_3): EncoderBlock(\n",
            "          (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "          (self_attention): MultiheadAttention(\n",
            "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
            "          )\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "          (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "          (mlp): MLPBlock(\n",
            "            (0): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (1): GELU(approximate='none')\n",
            "            (2): Dropout(p=0.0, inplace=False)\n",
            "            (3): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (4): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (encoder_layer_4): EncoderBlock(\n",
            "          (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "          (self_attention): MultiheadAttention(\n",
            "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
            "          )\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "          (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "          (mlp): MLPBlock(\n",
            "            (0): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (1): GELU(approximate='none')\n",
            "            (2): Dropout(p=0.0, inplace=False)\n",
            "            (3): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (4): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (encoder_layer_5): EncoderBlock(\n",
            "          (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "          (self_attention): MultiheadAttention(\n",
            "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
            "          )\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "          (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "          (mlp): MLPBlock(\n",
            "            (0): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (1): GELU(approximate='none')\n",
            "            (2): Dropout(p=0.0, inplace=False)\n",
            "            (3): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (4): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (encoder_layer_6): EncoderBlock(\n",
            "          (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "          (self_attention): MultiheadAttention(\n",
            "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
            "          )\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "          (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "          (mlp): MLPBlock(\n",
            "            (0): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (1): GELU(approximate='none')\n",
            "            (2): Dropout(p=0.0, inplace=False)\n",
            "            (3): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (4): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (encoder_layer_7): EncoderBlock(\n",
            "          (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "          (self_attention): MultiheadAttention(\n",
            "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
            "          )\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "          (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "          (mlp): MLPBlock(\n",
            "            (0): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (1): GELU(approximate='none')\n",
            "            (2): Dropout(p=0.0, inplace=False)\n",
            "            (3): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (4): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (encoder_layer_8): EncoderBlock(\n",
            "          (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "          (self_attention): MultiheadAttention(\n",
            "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
            "          )\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "          (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "          (mlp): MLPBlock(\n",
            "            (0): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (1): GELU(approximate='none')\n",
            "            (2): Dropout(p=0.0, inplace=False)\n",
            "            (3): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (4): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (encoder_layer_9): EncoderBlock(\n",
            "          (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "          (self_attention): MultiheadAttention(\n",
            "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
            "          )\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "          (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "          (mlp): MLPBlock(\n",
            "            (0): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (1): GELU(approximate='none')\n",
            "            (2): Dropout(p=0.0, inplace=False)\n",
            "            (3): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (4): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (encoder_layer_10): EncoderBlock(\n",
            "          (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "          (self_attention): MultiheadAttention(\n",
            "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
            "          )\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "          (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "          (mlp): MLPBlock(\n",
            "            (0): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (1): GELU(approximate='none')\n",
            "            (2): Dropout(p=0.0, inplace=False)\n",
            "            (3): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (4): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (encoder_layer_11): EncoderBlock(\n",
            "          (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "          (self_attention): MultiheadAttention(\n",
            "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
            "          )\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "          (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "          (mlp): MLPBlock(\n",
            "            (0): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (1): GELU(approximate='none')\n",
            "            (2): Dropout(p=0.0, inplace=False)\n",
            "            (3): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (4): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (ln): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "    )\n",
            "    (heads): Sequential(\n",
            "      (head): Linear(in_features=768, out_features=4, bias=True)\n",
            "    )\n",
            "  )\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "class BloodCellViT(nn.Module):\n",
        "    def __init__(self, num_classes=4):\n",
        "        super(BloodCellViT, self).__init__()\n",
        "        # Load pre-trained ViT-Base/16\n",
        "        weights = ViT_B_16_Weights.DEFAULT\n",
        "        self.vit = vit_b_16(weights=None)\n",
        "        num_ftrs = self.vit.heads.head.in_features\n",
        "        self.vit.heads.head = nn.Linear(num_ftrs, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.vit(x)\n",
        "\n",
        "\n",
        "num_classes = 4 # Based on the previous data loading step\n",
        "model = BloodCellViT(num_classes=num_classes)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "print(f\"Model initialized and moved to {device}\")\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cf857af2"
      },
      "source": [
        "## Performer model implementation\n",
        "\n",
        "### Subtask:\n",
        "Implement the Performer model for blood cell classification.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "71e4448e"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "def linear_attention(q, k, v):\n",
        "    attention_weights = torch.matmul(q, k.transpose(-2, -1))\n",
        "    attention_weights = F.softmax(attention_weights, dim=-1)\n",
        "    output = torch.matmul(attention_weights, v)\n",
        "    return output\n",
        "\n",
        "class PatchEmbedding(nn.Module):\n",
        "    def __init__(self, img_size=224, patch_size=16, in_channels=3, embed_dim=768):\n",
        "        super().__init__()\n",
        "        self.img_size = img_size\n",
        "        self.patch_size = patch_size\n",
        "        self.num_patches = (img_size // patch_size) ** 2\n",
        "        self.proj = nn.Conv2d(in_channels, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.proj(x)\n",
        "        x = x.flatten(2)\n",
        "        x = x.transpose(1, 2)\n",
        "        return x\n",
        "\n",
        "class PerformerBlock(nn.Module):\n",
        "    def __init__(self, embed_dim, num_heads, mlp_ratio=4., drop_rate=0., attn_drop_rate=0.):\n",
        "        super().__init__()\n",
        "        self.norm1 = nn.LayerNorm(embed_dim)\n",
        "        self.attn = nn.MultiheadAttention(embed_dim, num_heads, dropout=attn_drop_rate, batch_first=True) # Using standard MultiheadAttention for now, would replace with efficient attention\n",
        "        self.norm2 = nn.LayerNorm(embed_dim)\n",
        "        mlp_hidden_dim = int(embed_dim * mlp_ratio)\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(embed_dim, mlp_hidden_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(drop_rate),\n",
        "            nn.Linear(mlp_hidden_dim, embed_dim),\n",
        "            nn.Dropout(drop_rate)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.attn(self.norm1(x), self.norm1(x), self.norm1(x))[0]\n",
        "        x = x + self.mlp(self.norm2(x))\n",
        "        return x\n",
        "\n",
        "class PerformerModel(nn.Module):\n",
        "    def __init__(self, img_size=224, patch_size=16, in_channels=3, embed_dim=768, num_layers=12, num_heads=12, mlp_ratio=4., num_classes=4, drop_rate=0., attn_drop_rate=0.):\n",
        "        super().__init__()\n",
        "        self.num_classes = num_classes\n",
        "        self.embed_dim = embed_dim\n",
        "\n",
        "        self.patch_embed = PatchEmbedding(img_size, patch_size, in_channels, embed_dim)\n",
        "        num_patches = self.patch_embed.num_patches\n",
        "\n",
        "        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches + 1, embed_dim))\n",
        "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
        "        self.pos_drop = nn.Dropout(p=drop_rate)\n",
        "\n",
        "        self.blocks = nn.ModuleList([\n",
        "            PerformerBlock(embed_dim, num_heads, mlp_ratio, drop_rate, attn_drop_rate)\n",
        "            for _ in range(num_layers)\n",
        "        ])\n",
        "\n",
        "        self.norm = nn.LayerNorm(embed_dim)\n",
        "        self.head = nn.Linear(embed_dim, num_classes) if num_classes > 0 else nn.Identity()\n",
        "\n",
        "        # Initialize weights\n",
        "        nn.init.trunc_normal_(self.pos_embed, std=.02)\n",
        "        nn.init.trunc_normal_(self.cls_token, std=.02)\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, m):\n",
        "        if isinstance(m, nn.Linear):\n",
        "            nn.init.trunc_normal_(m.weight, std=.02)\n",
        "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "        elif isinstance(m, nn.LayerNorm):\n",
        "            nn.init.constant_(m.bias, 0)\n",
        "            nn.init.constant_(m.weight, 1.0)\n",
        "\n",
        "    def forward_features(self, x):\n",
        "        B = x.shape[0]\n",
        "        x = self.patch_embed(x)\n",
        "\n",
        "        cls_tokens = self.cls_token.expand(B, -1, -1)\n",
        "        x = torch.cat((cls_tokens, x), dim=1)\n",
        "\n",
        "        x = x + self.pos_embed\n",
        "        x = self.pos_drop(x)\n",
        "\n",
        "        for blk in self.blocks:\n",
        "            x = blk(x)\n",
        "\n",
        "        x = self.norm(x)\n",
        "        return x[:, 0]\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.forward_features(x)\n",
        "        x = self.head(x)\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0a321914",
        "outputId": "4381058b-dd34-4c44-e23b-500690b67d5d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Performer Model initialized and moved to cuda\n",
            "PerformerModel(\n",
            "  (patch_embed): PatchEmbedding(\n",
            "    (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
            "  )\n",
            "  (pos_drop): Dropout(p=0.0, inplace=False)\n",
            "  (blocks): ModuleList(\n",
            "    (0-11): 12 x PerformerBlock(\n",
            "      (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "      (attn): MultiheadAttention(\n",
            "        (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
            "      )\n",
            "      (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "      (mlp): Sequential(\n",
            "        (0): Linear(in_features=768, out_features=3072, bias=True)\n",
            "        (1): GELU(approximate='none')\n",
            "        (2): Dropout(p=0.0, inplace=False)\n",
            "        (3): Linear(in_features=3072, out_features=768, bias=True)\n",
            "        (4): Dropout(p=0.0, inplace=False)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "  (head): Linear(in_features=768, out_features=4, bias=True)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "\n",
        "img_size = IMG_HEIGHT\n",
        "patch_size = 16\n",
        "in_channels = 3\n",
        "embed_dim = 768\n",
        "num_layers = 12\n",
        "num_heads = 12\n",
        "mlp_ratio = 4\n",
        "num_classes = num_classes\n",
        "\n",
        "performer_model = PerformerModel(\n",
        "    img_size=img_size,\n",
        "    patch_size=patch_size,\n",
        "    in_channels=in_channels,\n",
        "    embed_dim=embed_dim,\n",
        "    num_layers=num_layers,\n",
        "    num_heads=num_heads,\n",
        "    mlp_ratio=mlp_ratio,\n",
        "    num_classes=num_classes\n",
        ")\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "performer_model.to(device)\n",
        "\n",
        "print(f\"Performer Model initialized and moved to {device}\")\n",
        "print(performer_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e25d02f9"
      },
      "source": [
        "## Model training\n",
        "\n",
        "### Subtask:\n",
        "Train both the Vision Transformer and Performer models on the training data.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Newly Initialized ViT Model"
      ],
      "metadata": {
        "id": "XkHFKZhWvaOR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "XzurHPcRpaif",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a7f67a71-8636-49b8-cc40-519b54217b77"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/5: 100%|██████████| 313/313 [06:24<00:00,  1.23s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/5] → Loss: 1.4277, Train Acc: 28.13%\n",
            "Validation Accuracy: 56.12%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/5: 100%|██████████| 313/313 [06:24<00:00,  1.23s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [2/5] → Loss: 0.8736, Train Acc: 62.50%\n",
            "Validation Accuracy: 83.21%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/5: 100%|██████████| 313/313 [06:24<00:00,  1.23s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [3/5] → Loss: 0.6239, Train Acc: 72.64%\n",
            "Validation Accuracy: 84.73%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4/5: 100%|██████████| 313/313 [06:25<00:00,  1.23s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [4/5] → Loss: 0.7036, Train Acc: 69.95%\n",
            "Validation Accuracy: 86.89%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5/5: 100%|██████████| 313/313 [06:28<00:00,  1.24s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [5/5] → Loss: 0.5512, Train Acc: 75.38%\n",
            "Validation Accuracy: 86.49%\n",
            "\n",
            "✅ Final ViT Test Accuracy: 84.74%\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision.models import vit_b_16, ViT_B_16_Weights\n",
        "from tqdm import tqdm\n",
        "\n",
        "# class BloodCellViT(nn.Module):\n",
        "#     def __init__(self, num_classes=4):\n",
        "#         super(BloodCellViT, self).__init__()\n",
        "#         # Pretrained ViT model\n",
        "#         weights = ViT_B_16_Weights.DEFAULT\n",
        "#         self.vit = vit_b_16(weights=weights)\n",
        "#         num_ftrs = self.vit.heads.head.in_features\n",
        "#         self.vit.heads.head = nn.Linear(num_ftrs, num_classes)\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         return self.vit(x)\n",
        "\n",
        "def train_vit(model, train_loader, val_loader, criterion, optimizer, device, epochs=5):\n",
        "    model.to(device)\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        running_loss, correct, total = 0, 0, 0\n",
        "\n",
        "        for images, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\"):\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            correct += (preds == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "\n",
        "        train_acc = 100 * correct / total\n",
        "        print(f\"Epoch [{epoch+1}/{epochs}] → Loss: {running_loss/len(train_loader):.4f}, Train Acc: {train_acc:.2f}%\")\n",
        "\n",
        "        val_acc = test_vit(model, val_loader, device)\n",
        "        print(f\"Validation Accuracy: {val_acc:.2f}%\")\n",
        "\n",
        "    return model\n",
        "\n",
        "def test_vit(model, test_loader, device):\n",
        "    model.eval()\n",
        "    correct, total = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in test_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            correct += (preds == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "    return 100 * correct / total\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "num_classes = 4\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "vit_model = BloodCellViT(num_classes)\n",
        "optimizer = optim.AdamW(vit_model.parameters(), lr=1e-4)\n",
        "\n",
        "vit_model = train_vit(vit_model, train_dataloader, val_dataloader, criterion, optimizer, device, epochs=5)\n",
        "vit_test_acc = test_vit(vit_model, test_dataloader, device)\n",
        "print(f\"\\n✅ Final ViT Test Accuracy: {vit_test_acc:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PGfmoVuWvJuK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **PreTrained ViT Model**"
      ],
      "metadata": {
        "id": "-LvukEj9vK9c"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c7e47edb-97a5-4716-9ae4-428b750766aa",
        "id": "iETPbvLPvFUe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/5: 100%|██████████| 313/313 [06:29<00:00,  1.24s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/5] → Loss: 0.5570, Train Acc: 75.55%\n",
            "Validation Accuracy: 91.93%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/5: 100%|██████████| 313/313 [06:27<00:00,  1.24s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [2/5] → Loss: 0.3417, Train Acc: 85.24%\n",
            "Validation Accuracy: 96.56%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/5: 100%|██████████| 313/313 [06:28<00:00,  1.24s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [3/5] → Loss: 0.3072, Train Acc: 87.06%\n",
            "Validation Accuracy: 96.24%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4/5: 100%|██████████| 313/313 [06:27<00:00,  1.24s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [4/5] → Loss: 0.2820, Train Acc: 87.87%\n",
            "Validation Accuracy: 96.64%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5/5: 100%|██████████| 313/313 [06:28<00:00,  1.24s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [5/5] → Loss: 0.2511, Train Acc: 89.58%\n",
            "Validation Accuracy: 97.04%\n",
            "\n",
            "✅ Final ViT Test Accuracy: 97.12%\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision.models import vit_b_16, ViT_B_16_Weights\n",
        "from tqdm import tqdm\n",
        "\n",
        "class BloodCellViT(nn.Module):\n",
        "    def __init__(self, num_classes=4):\n",
        "        super(BloodCellViT, self).__init__()\n",
        "        # Pretrained ViT model\n",
        "        weights = ViT_B_16_Weights.DEFAULT\n",
        "        self.vit = vit_b_16(weights=weights)\n",
        "        num_ftrs = self.vit.heads.head.in_features\n",
        "        self.vit.heads.head = nn.Linear(num_ftrs, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.vit(x)\n",
        "\n",
        "def train_vit(model, train_loader, val_loader, criterion, optimizer, device, epochs=5):\n",
        "    model.to(device)\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        running_loss, correct, total = 0, 0, 0\n",
        "\n",
        "        for images, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\"):\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            correct += (preds == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "\n",
        "        train_acc = 100 * correct / total\n",
        "        print(f\"Epoch [{epoch+1}/{epochs}] → Loss: {running_loss/len(train_loader):.4f}, Train Acc: {train_acc:.2f}%\")\n",
        "\n",
        "        val_acc = test_vit(model, val_loader, device)\n",
        "        print(f\"Validation Accuracy: {val_acc:.2f}%\")\n",
        "\n",
        "    return model\n",
        "\n",
        "def test_vit(model, test_loader, device):\n",
        "    model.eval()\n",
        "    correct, total = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in test_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            correct += (preds == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "    return 100 * correct / total\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "num_classes = 4\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "vit_model = BloodCellViT(num_classes)\n",
        "optimizer = optim.AdamW(vit_model.parameters(), lr=1e-4)\n",
        "\n",
        "vit_model = train_vit(vit_model, train_dataloader, val_dataloader, criterion, optimizer, device, epochs=5)\n",
        "vit_test_acc = test_vit(vit_model, test_dataloader, device)\n",
        "print(f\"\\n✅ Final ViT Test Accuracy: {vit_test_acc:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "def train_performer(model, train_loader, val_loader, criterion, optimizer, device, epochs=5):\n",
        "    model.to(device)\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        running_loss, correct, total = 0, 0, 0\n",
        "\n",
        "        for images, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\"):\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            correct += (preds == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "\n",
        "        train_acc = 100 * correct / total\n",
        "        print(f\"Epoch [{epoch+1}/{epochs}] → Loss: {running_loss/len(train_loader):.4f}, Train Acc: {train_acc:.2f}%\")\n",
        "\n",
        "        val_acc = test_performer(model, val_loader, device)\n",
        "        print(f\"Validation Accuracy: {val_acc:.2f}%\")\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "def test_performer(model, test_loader, device):\n",
        "    model.eval()\n",
        "    correct, total = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in test_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            correct += (preds == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "    return 100 * correct / total\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.AdamW(performer_model.parameters(), lr=1e-4)\n",
        "\n",
        "\n",
        "performer_model = train_performer(performer_model, train_dataloader, val_dataloader, criterion, optimizer, device, epochs=5)\n",
        "\n",
        "performer_test_acc = test_performer(performer_model, test_dataloader, device)\n",
        "print(f\"\\n✅ Final Performer Test Accuracy: {performer_test_acc:.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BZ7d8lmATmL5",
        "outputId": "08acdace-1299-4f86-bc65-b9bb5bcd3446"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/5: 100%|██████████| 313/313 [06:19<00:00,  1.21s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/5] → Loss: 1.4087, Train Acc: 30.07%\n",
            "Validation Accuracy: 46.52%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/5: 100%|██████████| 313/313 [06:19<00:00,  1.21s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [2/5] → Loss: 0.9530, Train Acc: 57.67%\n",
            "Validation Accuracy: 77.30%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/5: 100%|██████████| 313/313 [06:19<00:00,  1.21s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [3/5] → Loss: 0.6807, Train Acc: 70.94%\n",
            "Validation Accuracy: 83.37%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4/5: 100%|██████████| 313/313 [06:19<00:00,  1.21s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [4/5] → Loss: 0.5925, Train Acc: 74.37%\n",
            "Validation Accuracy: 87.77%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5/5: 100%|██████████| 313/313 [06:21<00:00,  1.22s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [5/5] → Loss: 0.5276, Train Acc: 76.98%\n",
            "Validation Accuracy: 88.09%\n",
            "\n",
            "✅ Final Performer Test Accuracy: 88.90%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "p_hI7PM9TmgQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "NU1JdNztv9WB"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Va3BFeGQTmjn"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}