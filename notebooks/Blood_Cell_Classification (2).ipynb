{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sU49HUdckIID",
        "outputId": "a1f8c525-3227-4ee0-b1db-2bc9e7c1332a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "fatal: destination path 'blood-cell-classification' already exists and is not an empty directory.\n",
            "/content/blood-cell-classification/blood-cell-classification\n"
          ]
        }
      ],
      "source": [
        "# !git clone https://github.com/ankitsunil530/blood-cell-classification.git\n",
        "# %cd blood-cell-classification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aiGH1t3MkQ8D",
        "outputId": "3119b7e3-ce71-43d5-fc1f-d12300e470d6"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'google.colab'",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgoogle\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcolab\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m drive\n\u001b[32m      2\u001b[39m drive.mount(\u001b[33m'\u001b[39m\u001b[33m/content/drive\u001b[39m\u001b[33m'\u001b[39m)\n",
            "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'google.colab'"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o77FxEgYMI7U"
      },
      "source": [
        "# **Blood Cell Classification**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c1bd6044"
      },
      "source": [
        "## Data loading and preprocessing\n",
        "\n",
        "### Subtask:\n",
        "Load the blood cell dataset and preprocess the images for use with the vision transformer and Performer models. This will involve resizing, normalization, and splitting the data into training, validation, and test sets.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "e819139e"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "caabfb6a"
      },
      "outputs": [],
      "source": [
        "DATASET_PATH = \"/content/drive/MyDrive/Deep Learning Lab/Blood_Cells_Dataset/Blood_Cells_Dataset\"\n",
        "IMG_HEIGHT = 224\n",
        "IMG_WIDTH = 224\n",
        "\n",
        "def load_images_and_labels(dataset_path):\n",
        "    images = []\n",
        "    labels = []\n",
        "    class_names = sorted(os.listdir(dataset_path))\n",
        "    class_to_idx = {class_name: i for i, class_name in enumerate(class_names)}\n",
        "\n",
        "    for class_name in class_names:\n",
        "        class_dir = os.path.join(dataset_path, class_name)\n",
        "        if os.path.isdir(class_dir):\n",
        "            for img_name in os.listdir(class_dir):\n",
        "                img_path = os.path.join(class_dir, img_name)\n",
        "                img = cv2.imread(img_path)\n",
        "                if img is not None:\n",
        "                    img = cv2.resize(img, (IMG_WIDTH, IMG_HEIGHT))\n",
        "                    images.append(img)\n",
        "                    labels.append(class_to_idx[class_name])\n",
        "\n",
        "    return np.array(images), np.array(labels), class_names\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5v91fwvHf3YN",
        "outputId": "50211248-6f50-496e-ad79-b335218cfc7a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Blood_Cells_Dataset/\n",
            "    dataset2-master/\n",
            "        dataset2-master/\n",
            "            images/\n",
            "                TEST_SIMPLE/\n",
            "                    MONOCYTE/\n",
            "                    NEUTROPHIL/\n",
            "                    LYMPHOCYTE/\n",
            "                    EOSINOPHIL/\n",
            "                TEST/\n",
            "                    NEUTROPHIL/\n",
            "                    MONOCYTE/\n",
            "                    LYMPHOCYTE/\n",
            "                    EOSINOPHIL/\n",
            "                TRAIN/\n",
            "                    NEUTROPHIL/\n",
            "                    MONOCYTE/\n",
            "                    LYMPHOCYTE/\n",
            "                    EOSINOPHIL/\n",
            "    dataset-master/\n",
            "        dataset-master/\n",
            "            Annotations/\n",
            "            JPEGImages/\n"
          ]
        }
      ],
      "source": [
        "for root, dirs, files in os.walk(DATASET_PATH):\n",
        "    level = root.replace(DATASET_PATH, '').count(os.sep)\n",
        "    indent = ' ' * 4 * (level)\n",
        "    print(f'{indent}{os.path.basename(root)}/')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c0b77a45",
        "outputId": "3107e169-b8e7-4831-c9dd-07ec7a07276f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found classes: ['EOSINOPHIL', 'LYMPHOCYTE', 'MONOCYTE', 'NEUTROPHIL']\n",
            "Processing directory: /content/drive/MyDrive/Deep Learning Lab/Blood_Cells_Dataset/Blood_Cells_Dataset/dataset2-master/dataset2-master/images/TRAIN/NEUTROPHIL\n",
            "Processing directory: /content/drive/MyDrive/Deep Learning Lab/Blood_Cells_Dataset/Blood_Cells_Dataset/dataset2-master/dataset2-master/images/TRAIN/MONOCYTE\n",
            "Processing directory: /content/drive/MyDrive/Deep Learning Lab/Blood_Cells_Dataset/Blood_Cells_Dataset/dataset2-master/dataset2-master/images/TRAIN/LYMPHOCYTE\n",
            "Processing directory: /content/drive/MyDrive/Deep Learning Lab/Blood_Cells_Dataset/Blood_Cells_Dataset/dataset2-master/dataset2-master/images/TRAIN/EOSINOPHIL\n",
            "Processing directory: /content/drive/MyDrive/Deep Learning Lab/Blood_Cells_Dataset/Blood_Cells_Dataset/dataset2-master/dataset2-master/images/TEST/NEUTROPHIL\n",
            "Processing directory: /content/drive/MyDrive/Deep Learning Lab/Blood_Cells_Dataset/Blood_Cells_Dataset/dataset2-master/dataset2-master/images/TEST/MONOCYTE\n",
            "Processing directory: /content/drive/MyDrive/Deep Learning Lab/Blood_Cells_Dataset/Blood_Cells_Dataset/dataset2-master/dataset2-master/images/TEST/LYMPHOCYTE\n",
            "Processing directory: /content/drive/MyDrive/Deep Learning Lab/Blood_Cells_Dataset/Blood_Cells_Dataset/dataset2-master/dataset2-master/images/TEST/EOSINOPHIL\n",
            "Processing directory: /content/drive/MyDrive/Deep Learning Lab/Blood_Cells_Dataset/Blood_Cells_Dataset/dataset2-master/dataset2-master/images/TEST_SIMPLE/MONOCYTE\n",
            "Processing directory: /content/drive/MyDrive/Deep Learning Lab/Blood_Cells_Dataset/Blood_Cells_Dataset/dataset2-master/dataset2-master/images/TEST_SIMPLE/NEUTROPHIL\n",
            "Processing directory: /content/drive/MyDrive/Deep Learning Lab/Blood_Cells_Dataset/Blood_Cells_Dataset/dataset2-master/dataset2-master/images/TEST_SIMPLE/LYMPHOCYTE\n",
            "Processing directory: /content/drive/MyDrive/Deep Learning Lab/Blood_Cells_Dataset/Blood_Cells_Dataset/dataset2-master/dataset2-master/images/TEST_SIMPLE/EOSINOPHIL\n",
            "Loaded 12515 images and 12515 labels.\n",
            "Training set size: 10012\n",
            "Validation set size: 1251\n",
            "Test set size: 1252\n",
            "Class names: ['EOSINOPHIL', 'LYMPHOCYTE', 'MONOCYTE', 'NEUTROPHIL']\n"
          ]
        }
      ],
      "source": [
        "def load_images_and_labels(dataset_path):\n",
        "    images = []\n",
        "    labels = []\n",
        "    # Update base directories to reflect the actual structure\n",
        "    base_image_dirs = [\n",
        "        os.path.join('dataset2-master', 'dataset2-master', 'images', 'TRAIN'),\n",
        "        os.path.join('dataset2-master', 'dataset2-master', 'images', 'TEST'),\n",
        "        os.path.join('dataset2-master', 'dataset2-master', 'images', 'TEST_SIMPLE')\n",
        "    ]\n",
        "    class_names = set()\n",
        "\n",
        "    # First pass to collect all unique class names\n",
        "    for base_img_dir_name in base_image_dirs:\n",
        "        base_img_dir_path = os.path.join(dataset_path, base_img_dir_name)\n",
        "        if os.path.isdir(base_img_dir_path):\n",
        "            for class_name in os.listdir(base_img_dir_path):\n",
        "                class_dir_path = os.path.join(base_img_dir_path, class_name)\n",
        "                if os.path.isdir(class_dir_path):\n",
        "                    class_names.add(class_name)\n",
        "\n",
        "    class_names = sorted(list(class_names))\n",
        "    class_to_idx = {class_name: i for i, class_name in enumerate(class_names)}\n",
        "\n",
        "    print(f\"Found classes: {class_names}\")\n",
        "\n",
        "    # Second pass to load images\n",
        "    for base_img_dir_name in base_image_dirs:\n",
        "        base_img_dir_path = os.path.join(dataset_path, base_img_dir_name)\n",
        "        if os.path.isdir(base_img_dir_path):\n",
        "            for class_name in os.listdir(base_img_dir_path):\n",
        "                class_dir_path = os.path.join(base_img_dir_path, class_name)\n",
        "                print(f\"Processing directory: {class_dir_path}\")\n",
        "                if os.path.isdir(class_dir_path) and class_name in class_to_idx:\n",
        "                    for img_name in os.listdir(class_dir_path):\n",
        "                        img_path = os.path.join(class_dir_path, img_name)\n",
        "                        if os.path.isfile(img_path) and img_name.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
        "                            img = cv2.imread(img_path)\n",
        "                            if img is not None:\n",
        "                                img = cv2.resize(img, (IMG_WIDTH, IMG_HEIGHT))\n",
        "                                images.append(img)\n",
        "                                labels.append(class_to_idx[class_name])\n",
        "                            else:\n",
        "                                print(f\"Could not load image: {img_path}\")\n",
        "\n",
        "    print(f\"Loaded {len(images)} images and {len(labels)} labels.\")\n",
        "    return np.array(images), np.array(labels), class_names\n",
        "\n",
        "images, labels, class_names = load_images_and_labels(DATASET_PATH)\n",
        "\n",
        "# Split the data - combining images from TRAIN, TEST, TEST_SIMPLE before splitting\n",
        "# Using a fixed random state for reproducibility\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(images, labels, test_size=0.2, stratify=labels, random_state=42)\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, stratify=y_temp, random_state=42)\n",
        "\n",
        "print(f\"Training set size: {len(X_train)}\")\n",
        "print(f\"Validation set size: {len(X_val)}\")\n",
        "print(f\"Test set size: {len(X_test)}\")\n",
        "print(f\"Class names: {class_names}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d77d6a6a"
      },
      "source": [
        "## Data loading and preprocessing\n",
        "\n",
        "### Subtask:\n",
        "Load the blood cell dataset and preprocess the images for use with the vision transformer and Performer models. This will involve resizing, normalization, and splitting the data into training, validation, and test sets.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "96d53a02"
      },
      "outputs": [],
      "source": [
        "class BloodCellDataset(Dataset):\n",
        "    def __init__(self, images, labels, transform=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            images (numpy array): Array of image data (H, W, C).\n",
        "            labels (numpy array): Array of corresponding labels.\n",
        "            transform (callable, optional): Optional transform to be applied\n",
        "                on a sample.\n",
        "        \"\"\"\n",
        "        self.images = images\n",
        "        self.labels = labels\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if torch.is_tensor(idx):\n",
        "            idx = idx.tolist()\n",
        "\n",
        "        image = self.images[idx]\n",
        "        label = self.labels[idx]\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, label\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "25837724",
        "outputId": "3f5858ea-30d4-49e7-b7f4-346399aceee2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of training batches: 313\n",
            "Number of validation batches: 40\n",
            "Number of test batches: 40\n"
          ]
        }
      ],
      "source": [
        "# Define transformations\n",
        "# Using common mean and std dev for ImageNet as a starting point\n",
        "# A more accurate approach would be to calculate these from the dataset\n",
        "mean = [0.485, 0.456, 0.406]\n",
        "std = [0.229, 0.224, 0.225]\n",
        "\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.ToPILImage(), # Convert numpy array to PIL Image for transforms\n",
        "    transforms.RandomResizedCrop(IMG_HEIGHT),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=mean, std=std),\n",
        "])\n",
        "\n",
        "val_test_transform = transforms.Compose([\n",
        "    transforms.ToPILImage(), # Convert numpy array to PIL Image for transforms\n",
        "    transforms.Resize(IMG_HEIGHT),\n",
        "    transforms.CenterCrop(IMG_HEIGHT),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=mean, std=std),\n",
        "])\n",
        "\n",
        "# Create Dataset instances\n",
        "train_dataset = BloodCellDataset(X_train, y_train, transform=train_transform)\n",
        "val_dataset = BloodCellDataset(X_val, y_val, transform=val_test_transform)\n",
        "test_dataset = BloodCellDataset(X_test, y_test, transform=val_test_transform)\n",
        "\n",
        "# Create DataLoader instances\n",
        "batch_size = 32 # Using the already defined batch_size\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "print(f\"Number of training batches: {len(train_dataloader)}\")\n",
        "print(f\"Number of validation batches: {len(val_dataloader)}\")\n",
        "print(f\"Number of test batches: {len(test_dataloader)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7757e7a6"
      },
      "source": [
        "## Vision transformer model implementation\n",
        "\n",
        "### Subtask:\n",
        "Implement the Vision Transformer model for blood cell classification.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "59d19bcc"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "from torchvision.models import vit_b_16, ViT_B_16_Weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7cae6304",
        "outputId": "9b62c8e1-8c62-4646-ce42-d544cfe90012"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model initialized and moved to cuda\n",
            "BloodCellViT(\n",
            "  (vit): VisionTransformer(\n",
            "    (conv_proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
            "    (encoder): Encoder(\n",
            "      (dropout): Dropout(p=0.0, inplace=False)\n",
            "      (layers): Sequential(\n",
            "        (encoder_layer_0): EncoderBlock(\n",
            "          (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "          (self_attention): MultiheadAttention(\n",
            "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
            "          )\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "          (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "          (mlp): MLPBlock(\n",
            "            (0): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (1): GELU(approximate='none')\n",
            "            (2): Dropout(p=0.0, inplace=False)\n",
            "            (3): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (4): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (encoder_layer_1): EncoderBlock(\n",
            "          (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "          (self_attention): MultiheadAttention(\n",
            "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
            "          )\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "          (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "          (mlp): MLPBlock(\n",
            "            (0): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (1): GELU(approximate='none')\n",
            "            (2): Dropout(p=0.0, inplace=False)\n",
            "            (3): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (4): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (encoder_layer_2): EncoderBlock(\n",
            "          (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "          (self_attention): MultiheadAttention(\n",
            "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
            "          )\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "          (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "          (mlp): MLPBlock(\n",
            "            (0): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (1): GELU(approximate='none')\n",
            "            (2): Dropout(p=0.0, inplace=False)\n",
            "            (3): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (4): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (encoder_layer_3): EncoderBlock(\n",
            "          (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "          (self_attention): MultiheadAttention(\n",
            "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
            "          )\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "          (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "          (mlp): MLPBlock(\n",
            "            (0): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (1): GELU(approximate='none')\n",
            "            (2): Dropout(p=0.0, inplace=False)\n",
            "            (3): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (4): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (encoder_layer_4): EncoderBlock(\n",
            "          (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "          (self_attention): MultiheadAttention(\n",
            "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
            "          )\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "          (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "          (mlp): MLPBlock(\n",
            "            (0): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (1): GELU(approximate='none')\n",
            "            (2): Dropout(p=0.0, inplace=False)\n",
            "            (3): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (4): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (encoder_layer_5): EncoderBlock(\n",
            "          (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "          (self_attention): MultiheadAttention(\n",
            "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
            "          )\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "          (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "          (mlp): MLPBlock(\n",
            "            (0): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (1): GELU(approximate='none')\n",
            "            (2): Dropout(p=0.0, inplace=False)\n",
            "            (3): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (4): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (encoder_layer_6): EncoderBlock(\n",
            "          (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "          (self_attention): MultiheadAttention(\n",
            "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
            "          )\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "          (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "          (mlp): MLPBlock(\n",
            "            (0): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (1): GELU(approximate='none')\n",
            "            (2): Dropout(p=0.0, inplace=False)\n",
            "            (3): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (4): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (encoder_layer_7): EncoderBlock(\n",
            "          (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "          (self_attention): MultiheadAttention(\n",
            "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
            "          )\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "          (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "          (mlp): MLPBlock(\n",
            "            (0): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (1): GELU(approximate='none')\n",
            "            (2): Dropout(p=0.0, inplace=False)\n",
            "            (3): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (4): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (encoder_layer_8): EncoderBlock(\n",
            "          (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "          (self_attention): MultiheadAttention(\n",
            "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
            "          )\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "          (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "          (mlp): MLPBlock(\n",
            "            (0): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (1): GELU(approximate='none')\n",
            "            (2): Dropout(p=0.0, inplace=False)\n",
            "            (3): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (4): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (encoder_layer_9): EncoderBlock(\n",
            "          (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "          (self_attention): MultiheadAttention(\n",
            "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
            "          )\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "          (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "          (mlp): MLPBlock(\n",
            "            (0): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (1): GELU(approximate='none')\n",
            "            (2): Dropout(p=0.0, inplace=False)\n",
            "            (3): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (4): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (encoder_layer_10): EncoderBlock(\n",
            "          (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "          (self_attention): MultiheadAttention(\n",
            "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
            "          )\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "          (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "          (mlp): MLPBlock(\n",
            "            (0): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (1): GELU(approximate='none')\n",
            "            (2): Dropout(p=0.0, inplace=False)\n",
            "            (3): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (4): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (encoder_layer_11): EncoderBlock(\n",
            "          (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "          (self_attention): MultiheadAttention(\n",
            "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
            "          )\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "          (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "          (mlp): MLPBlock(\n",
            "            (0): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (1): GELU(approximate='none')\n",
            "            (2): Dropout(p=0.0, inplace=False)\n",
            "            (3): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (4): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (ln): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "    )\n",
            "    (heads): Sequential(\n",
            "      (head): Linear(in_features=768, out_features=4, bias=True)\n",
            "    )\n",
            "  )\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "class BloodCellViT(nn.Module):\n",
        "    def __init__(self, num_classes=4):\n",
        "        super(BloodCellViT, self).__init__()\n",
        "        # Load pre-trained ViT-Base/16\n",
        "        weights = ViT_B_16_Weights.DEFAULT\n",
        "        self.vit = vit_b_16(weights=None)\n",
        "        num_ftrs = self.vit.heads.head.in_features\n",
        "        self.vit.heads.head = nn.Linear(num_ftrs, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.vit(x)\n",
        "\n",
        "\n",
        "num_classes = 4 # Based on the previous data loading step\n",
        "model = BloodCellViT(num_classes=num_classes)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "print(f\"Model initialized and moved to {device}\")\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cf857af2"
      },
      "source": [
        "## Performer model implementation\n",
        "\n",
        "### Subtask:\n",
        "Implement the Performer model for blood cell classification.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "71e4448e"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "def linear_attention(q, k, v):\n",
        "    attention_weights = torch.matmul(q, k.transpose(-2, -1))\n",
        "    attention_weights = F.softmax(attention_weights, dim=-1)\n",
        "    output = torch.matmul(attention_weights, v)\n",
        "    return output\n",
        "\n",
        "class PatchEmbedding(nn.Module):\n",
        "    def __init__(self, img_size=224, patch_size=16, in_channels=3, embed_dim=768):\n",
        "        super().__init__()\n",
        "        self.img_size = img_size\n",
        "        self.patch_size = patch_size\n",
        "        self.num_patches = (img_size // patch_size) ** 2\n",
        "        self.proj = nn.Conv2d(in_channels, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.proj(x)\n",
        "        x = x.flatten(2)\n",
        "        x = x.transpose(1, 2)\n",
        "        return x\n",
        "\n",
        "class PerformerBlock(nn.Module):\n",
        "    def __init__(self, embed_dim, num_heads, mlp_ratio=4., drop_rate=0., attn_drop_rate=0.):\n",
        "        super().__init__()\n",
        "        self.norm1 = nn.LayerNorm(embed_dim)\n",
        "        self.attn = nn.MultiheadAttention(embed_dim, num_heads, dropout=attn_drop_rate, batch_first=True) # Using standard MultiheadAttention for now, would replace with efficient attention\n",
        "        self.norm2 = nn.LayerNorm(embed_dim)\n",
        "        mlp_hidden_dim = int(embed_dim * mlp_ratio)\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(embed_dim, mlp_hidden_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(drop_rate),\n",
        "            nn.Linear(mlp_hidden_dim, embed_dim),\n",
        "            nn.Dropout(drop_rate)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.attn(self.norm1(x), self.norm1(x), self.norm1(x))[0]\n",
        "        x = x + self.mlp(self.norm2(x))\n",
        "        return x\n",
        "\n",
        "class PerformerModel(nn.Module):\n",
        "    def __init__(self, img_size=224, patch_size=16, in_channels=3, embed_dim=768, num_layers=12, num_heads=12, mlp_ratio=4., num_classes=4, drop_rate=0., attn_drop_rate=0.):\n",
        "        super().__init__()\n",
        "        self.num_classes = num_classes\n",
        "        self.embed_dim = embed_dim\n",
        "\n",
        "        self.patch_embed = PatchEmbedding(img_size, patch_size, in_channels, embed_dim)\n",
        "        num_patches = self.patch_embed.num_patches\n",
        "\n",
        "        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches + 1, embed_dim))\n",
        "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
        "        self.pos_drop = nn.Dropout(p=drop_rate)\n",
        "\n",
        "        self.blocks = nn.ModuleList([\n",
        "            PerformerBlock(embed_dim, num_heads, mlp_ratio, drop_rate, attn_drop_rate)\n",
        "            for _ in range(num_layers)\n",
        "        ])\n",
        "\n",
        "        self.norm = nn.LayerNorm(embed_dim)\n",
        "        self.head = nn.Linear(embed_dim, num_classes) if num_classes > 0 else nn.Identity()\n",
        "\n",
        "        # Initialize weights\n",
        "        nn.init.trunc_normal_(self.pos_embed, std=.02)\n",
        "        nn.init.trunc_normal_(self.cls_token, std=.02)\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, m):\n",
        "        if isinstance(m, nn.Linear):\n",
        "            nn.init.trunc_normal_(m.weight, std=.02)\n",
        "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "        elif isinstance(m, nn.LayerNorm):\n",
        "            nn.init.constant_(m.bias, 0)\n",
        "            nn.init.constant_(m.weight, 1.0)\n",
        "\n",
        "    def forward_features(self, x):\n",
        "        B = x.shape[0]\n",
        "        x = self.patch_embed(x)\n",
        "\n",
        "        cls_tokens = self.cls_token.expand(B, -1, -1)\n",
        "        x = torch.cat((cls_tokens, x), dim=1)\n",
        "\n",
        "        x = x + self.pos_embed\n",
        "        x = self.pos_drop(x)\n",
        "\n",
        "        for blk in self.blocks:\n",
        "            x = blk(x)\n",
        "\n",
        "        x = self.norm(x)\n",
        "        return x[:, 0]\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.forward_features(x)\n",
        "        x = self.head(x)\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0a321914",
        "outputId": "4381058b-dd34-4c44-e23b-500690b67d5d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Performer Model initialized and moved to cuda\n",
            "PerformerModel(\n",
            "  (patch_embed): PatchEmbedding(\n",
            "    (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
            "  )\n",
            "  (pos_drop): Dropout(p=0.0, inplace=False)\n",
            "  (blocks): ModuleList(\n",
            "    (0-11): 12 x PerformerBlock(\n",
            "      (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "      (attn): MultiheadAttention(\n",
            "        (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
            "      )\n",
            "      (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "      (mlp): Sequential(\n",
            "        (0): Linear(in_features=768, out_features=3072, bias=True)\n",
            "        (1): GELU(approximate='none')\n",
            "        (2): Dropout(p=0.0, inplace=False)\n",
            "        (3): Linear(in_features=3072, out_features=768, bias=True)\n",
            "        (4): Dropout(p=0.0, inplace=False)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "  (head): Linear(in_features=768, out_features=4, bias=True)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "\n",
        "img_size = IMG_HEIGHT\n",
        "patch_size = 16\n",
        "in_channels = 3\n",
        "embed_dim = 768\n",
        "num_layers = 12\n",
        "num_heads = 12\n",
        "mlp_ratio = 4\n",
        "num_classes = num_classes\n",
        "\n",
        "performer_model = PerformerModel(\n",
        "    img_size=img_size,\n",
        "    patch_size=patch_size,\n",
        "    in_channels=in_channels,\n",
        "    embed_dim=embed_dim,\n",
        "    num_layers=num_layers,\n",
        "    num_heads=num_heads,\n",
        "    mlp_ratio=mlp_ratio,\n",
        "    num_classes=num_classes\n",
        ")\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "performer_model.to(device)\n",
        "\n",
        "print(f\"Performer Model initialized and moved to {device}\")\n",
        "print(performer_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e25d02f9"
      },
      "source": [
        "## Model training\n",
        "\n",
        "### Subtask:\n",
        "Train both the Vision Transformer and Performer models on the training data.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XkHFKZhWvaOR"
      },
      "source": [
        "# Newly Initialized ViT Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XzurHPcRpaif",
        "outputId": "a7f67a71-8636-49b8-cc40-519b54217b77"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1/5: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 313/313 [06:24<00:00,  1.23s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/5] â†’ Loss: 1.4277, Train Acc: 28.13%\n",
            "Validation Accuracy: 56.12%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2/5: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 313/313 [06:24<00:00,  1.23s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [2/5] â†’ Loss: 0.8736, Train Acc: 62.50%\n",
            "Validation Accuracy: 83.21%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 3/5: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 313/313 [06:24<00:00,  1.23s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [3/5] â†’ Loss: 0.6239, Train Acc: 72.64%\n",
            "Validation Accuracy: 84.73%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 4/5: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 313/313 [06:25<00:00,  1.23s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [4/5] â†’ Loss: 0.7036, Train Acc: 69.95%\n",
            "Validation Accuracy: 86.89%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 5/5: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 313/313 [06:28<00:00,  1.24s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [5/5] â†’ Loss: 0.5512, Train Acc: 75.38%\n",
            "Validation Accuracy: 86.49%\n",
            "\n",
            "âœ… Final ViT Test Accuracy: 84.74%\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision.models import vit_b_16, ViT_B_16_Weights\n",
        "from tqdm import tqdm\n",
        "\n",
        "def train_vit(model, train_loader, val_loader, criterion, optimizer, device, epochs=5):\n",
        "    model.to(device)\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        running_loss, correct, total = 0, 0, 0\n",
        "\n",
        "        for images, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\"):\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            correct += (preds == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "\n",
        "        train_acc = 100 * correct / total\n",
        "        print(f\"Epoch [{epoch+1}/{epochs}] â†’ Loss: {running_loss/len(train_loader):.4f}, Train Acc: {train_acc:.2f}%\")\n",
        "\n",
        "        val_acc = test_vit(model, val_loader, device)\n",
        "        print(f\"Validation Accuracy: {val_acc:.2f}%\")\n",
        "\n",
        "    return model\n",
        "\n",
        "def test_vit(model, test_loader, device):\n",
        "    model.eval()\n",
        "    correct, total = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in test_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            correct += (preds == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "    return 100 * correct / total\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "num_classes = 4\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "vit_model = BloodCellViT(num_classes)\n",
        "optimizer = optim.AdamW(vit_model.parameters(), lr=1e-4)\n",
        "\n",
        "vit_model = train_vit(vit_model, train_dataloader, val_dataloader, criterion, optimizer, device, epochs=5)\n",
        "vit_test_acc = test_vit(vit_model, test_dataloader, device)\n",
        "print(f\"\\nâœ… Final ViT Test Accuracy: {vit_test_acc:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "PGfmoVuWvJuK"
      },
      "outputs": [
        {
          "ename": "ImportError",
          "evalue": "cannot import name '_cuda' from 'torch._utils' (c:\\Users\\Sunil Kumar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\_utils.py)",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[15], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmlflow\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmlflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpytorch\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\Sunil Kumar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\__init__.py:1215\u001b[0m\n\u001b[0;32m   1210\u001b[0m \u001b[38;5;66;03m################################################################################\u001b[39;00m\n\u001b[0;32m   1211\u001b[0m \u001b[38;5;66;03m# Define Storage and Tensor classes\u001b[39;00m\n\u001b[0;32m   1212\u001b[0m \u001b[38;5;66;03m################################################################################\u001b[39;00m\n\u001b[0;32m   1214\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_tensor\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Tensor\n\u001b[1;32m-> 1215\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstorage\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _StorageBase, TypedStorage, _LegacyStorage, UntypedStorage, _warn_typed_storage_removal\n\u001b[0;32m   1217\u001b[0m \u001b[38;5;66;03m# NOTE: New <type>Storage classes should never be added. When adding a new\u001b[39;00m\n\u001b[0;32m   1218\u001b[0m \u001b[38;5;66;03m# dtype, use torch.storage.TypedStorage directly.\u001b[39;00m\n\u001b[0;32m   1220\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mByteStorage\u001b[39;00m(_LegacyStorage):\n",
            "File \u001b[1;32mc:\\Users\\Sunil Kumar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\storage.py:4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mio\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _type, _cuda, _hpu\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtypes\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Storage\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m cast, Any, Dict \u001b[38;5;28;01mas\u001b[39;00m _Dict, Optional \u001b[38;5;28;01mas\u001b[39;00m _Optional, TypeVar, Type, Union\n",
            "\u001b[1;31mImportError\u001b[0m: cannot import name '_cuda' from 'torch._utils' (c:\\Users\\Sunil Kumar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\_utils.py)"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import mlflow\n",
        "import mlflow.pytorch\n",
        "\n",
        "# Assume 'performer_model' is your trained model object\n",
        "performer_model.eval()  # Set to evaluation mode\n",
        "\n",
        "# ðŸ”¹ 1. Save model to disk\n",
        "torch.save(performer_model.state_dict(), \"models/performer_model.pth\")\n",
        "print(\"âœ… Performer model saved to disk as performer_model.pth\")\n",
        "\n",
        "# ðŸ”¹ 2. Log model to MLflow\n",
        "mlflow.set_tracking_uri(\"file:///D:/Blood Cell Classifiaction/blood-cell-classification/mlruns\")\n",
        "mlflow.set_experiment(\"Blood-Cell-Classification-Results\")\n",
        "\n",
        "with mlflow.start_run(run_name=\"Performer-Model-Logged\"):\n",
        "    mlflow.pytorch.log_model(performer_model, artifact_path=\"performer_model\")\n",
        "    \n",
        "    # Log parameters\n",
        "    mlflow.log_param(\"model_type\", \"Performer (Efficient Transformer)\")\n",
        "    mlflow.log_param(\"epochs\", 5)\n",
        "    mlflow.log_param(\"optimizer\", \"AdamW\")\n",
        "    mlflow.log_param(\"learning_rate\", 1e-4)\n",
        "    mlflow.log_param(\"batch_size\", 32)\n",
        "    mlflow.log_param(\"num_layers\", 12)\n",
        "    mlflow.log_param(\"embed_dim\", 768)\n",
        "    mlflow.log_param(\"num_heads\", 12)\n",
        "    \n",
        "    # ðŸ”¹ 3. Log metrics per epoch\n",
        "    epoch_metrics = [\n",
        "        {\"loss\":1.4277, \"train_acc\":28.13, \"val_acc\":56.12},\n",
        "        {\"loss\":0.8736, \"train_acc\":62.50, \"val_acc\":83.21},\n",
        "        {\"loss\":0.6239, \"train_acc\":72.64, \"val_acc\":84.73},\n",
        "        {\"loss\":0.7036, \"train_acc\":69.95, \"val_acc\":86.89},\n",
        "        {\"loss\":0.5512, \"train_acc\":75.38, \"val_acc\":86.49}\n",
        "    ]\n",
        "    \n",
        "    for i, metrics in enumerate(epoch_metrics, start=1):\n",
        "        mlflow.log_metric(\"loss\", metrics[\"loss\"], step=i)\n",
        "        mlflow.log_metric(\"train_acc\", metrics[\"train_acc\"], step=i)\n",
        "        mlflow.log_metric(\"val_acc\", metrics[\"val_acc\"], step=i)\n",
        "    \n",
        "    # Final test accuracy\n",
        "    mlflow.log_metric(\"final_test_accuracy\", 84.74)\n",
        "\n",
        "print(\"âœ… Performer model logged to MLflow successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-LvukEj9vK9c"
      },
      "source": [
        "# **PreTrained ViT Model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iETPbvLPvFUe",
        "outputId": "c7e47edb-97a5-4716-9ae4-428b750766aa"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1/5: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 313/313 [06:29<00:00,  1.24s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/5] â†’ Loss: 0.5570, Train Acc: 75.55%\n",
            "Validation Accuracy: 91.93%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2/5: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 313/313 [06:27<00:00,  1.24s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [2/5] â†’ Loss: 0.3417, Train Acc: 85.24%\n",
            "Validation Accuracy: 96.56%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 3/5: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 313/313 [06:28<00:00,  1.24s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [3/5] â†’ Loss: 0.3072, Train Acc: 87.06%\n",
            "Validation Accuracy: 96.24%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 4/5: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 313/313 [06:27<00:00,  1.24s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [4/5] â†’ Loss: 0.2820, Train Acc: 87.87%\n",
            "Validation Accuracy: 96.64%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 5/5: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 313/313 [06:28<00:00,  1.24s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [5/5] â†’ Loss: 0.2511, Train Acc: 89.58%\n",
            "Validation Accuracy: 97.04%\n",
            "\n",
            "âœ… Final ViT Test Accuracy: 97.12%\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision.models import vit_b_16, ViT_B_16_Weights\n",
        "from tqdm import tqdm\n",
        "import mlflow\n",
        "import mlflow.pytorch\n",
        "\n",
        "class BloodCellViT(nn.Module):\n",
        "    def __init__(self, num_classes=4):\n",
        "        super(BloodCellViT, self).__init__()\n",
        "        # Pretrained ViT model\n",
        "        weights = ViT_B_16_Weights.DEFAULT\n",
        "        self.vit = vit_b_16(weights=weights)\n",
        "        num_ftrs = self.vit.heads.head.in_features\n",
        "        self.vit.heads.head = nn.Linear(num_ftrs, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.vit(x)\n",
        "\n",
        "def test_vit(model, test_loader, device):\n",
        "    model.eval()\n",
        "    correct, total = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in test_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            correct += (preds == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "    return 100 * correct / total\n",
        "\n",
        "def train_vit(model, train_loader, val_loader, criterion, optimizer, device, epochs=5):\n",
        "    model.to(device)\n",
        "    \n",
        "    # START MLFLOW RUN\n",
        "    with mlflow.start_run(run_name=f\"ViT-PreTrained-{epochs}epochs\"):\n",
        "        \n",
        "        # Log hyperparameters\n",
        "        mlflow.log_param(\"model_type\", \"Vision Transformer (PreTrained)\")\n",
        "        mlflow.log_param(\"epochs\", epochs)\n",
        "        mlflow.log_param(\"optimizer\", \"AdamW\")\n",
        "        mlflow.log_param(\"batch_size\", train_loader.batch_size)\n",
        "        mlflow.log_param(\"device\", str(device))\n",
        "        mlflow.log_param(\"learning_rate\", 1e-4)\n",
        "        \n",
        "        print(f\"\\nðŸš€ Starting ViT Training with MLflow tracking...\")\n",
        "        print(f\"   Run name: ViT-PreTrained-{epochs}epochs\")\n",
        "        \n",
        "        for epoch in range(epochs):\n",
        "            model.train()\n",
        "            running_loss, correct, total = 0, 0, 0\n",
        "\n",
        "            for images, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs} [TRAIN]\"):\n",
        "                images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "                outputs = model(images)\n",
        "                loss = criterion(outputs, labels)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "                running_loss += loss.item()\n",
        "                _, preds = torch.max(outputs, 1)\n",
        "                correct += (preds == labels).sum().item()\n",
        "                total += labels.size(0)\n",
        "\n",
        "            train_acc = 100 * correct / total\n",
        "            train_loss = running_loss / len(train_loader)\n",
        "            \n",
        "            # Validation\n",
        "            val_acc = test_vit(model, val_loader, device)\n",
        "            \n",
        "            print(f\"Epoch [{epoch+1}/{epochs}] â†’ Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%, Val Acc: {val_acc:.2f}%\")\n",
        "            \n",
        "            # Log metrics to MLflow\n",
        "            mlflow.log_metric(\"train_loss\", train_loss, step=epoch)\n",
        "            mlflow.log_metric(\"train_accuracy\", train_acc, step=epoch)\n",
        "            mlflow.log_metric(\"val_accuracy\", val_acc, step=epoch)\n",
        "        \n",
        "        # SAVE AND LOG THE MODEL\n",
        "        print(f\"\\nðŸ’¾ Saving model to MLflow...\")\n",
        "        mlflow.pytorch.log_model(model, artifact_path=\"vit_model\")\n",
        "        \n",
        "        print(f\"âœ… MLflow run completed! View at http://localhost:5000\")\n",
        "        \n",
        "    return model\n",
        "\n",
        "# Initialize\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "num_classes = 4\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "print(f\"\\nðŸ“Š Model Setup:\")\n",
        "print(f\"   Device: {device}\")\n",
        "print(f\"   Classes: {num_classes}\")\n",
        "\n",
        "vit_model = BloodCellViT(num_classes)\n",
        "optimizer = optim.AdamW(vit_model.parameters(), lr=1e-4)\n",
        "\n",
        "# Train with MLflow tracking\n",
        "vit_model = train_vit(vit_model, train_dataloader, val_dataloader, criterion, optimizer, device, epochs=5)\n",
        "\n",
        "# Final test\n",
        "vit_test_acc = test_vit(vit_model, test_dataloader, device)\n",
        "print(f\"\\nâœ… Final ViT Test Accuracy: {vit_test_acc:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BZ7d8lmATmL5",
        "outputId": "08acdace-1299-4f86-bc65-b9bb5bcd3446"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1/5: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 313/313 [06:19<00:00,  1.21s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/5] â†’ Loss: 1.4087, Train Acc: 30.07%\n",
            "Validation Accuracy: 46.52%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2/5: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 313/313 [06:19<00:00,  1.21s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [2/5] â†’ Loss: 0.9530, Train Acc: 57.67%\n",
            "Validation Accuracy: 77.30%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 3/5: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 313/313 [06:19<00:00,  1.21s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [3/5] â†’ Loss: 0.6807, Train Acc: 70.94%\n",
            "Validation Accuracy: 83.37%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 4/5: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 313/313 [06:19<00:00,  1.21s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [4/5] â†’ Loss: 0.5925, Train Acc: 74.37%\n",
            "Validation Accuracy: 87.77%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 5/5: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 313/313 [06:21<00:00,  1.22s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [5/5] â†’ Loss: 0.5276, Train Acc: 76.98%\n",
            "Validation Accuracy: 88.09%\n",
            "\n",
            "âœ… Final Performer Test Accuracy: 88.90%\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from tqdm import tqdm\n",
        "import mlflow\n",
        "import mlflow.pytorch\n",
        "\n",
        "def test_performer(model, test_loader, device):\n",
        "    model.eval()\n",
        "    correct, total = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in test_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            correct += (preds == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "    return 100 * correct / total\n",
        "\n",
        "def train_performer(model, train_loader, val_loader, criterion, optimizer, device, epochs=5):\n",
        "    model.to(device)\n",
        "    \n",
        "    # START MLFLOW RUN\n",
        "    with mlflow.start_run(run_name=f\"Performer-{epochs}epochs\"):\n",
        "        \n",
        "        # Log hyperparameters\n",
        "        mlflow.log_param(\"model_type\", \"Performer\")\n",
        "        mlflow.log_param(\"epochs\", epochs)\n",
        "        mlflow.log_param(\"optimizer\", \"AdamW\")\n",
        "        mlflow.log_param(\"batch_size\", train_loader.batch_size)\n",
        "        mlflow.log_param(\"device\", str(device))\n",
        "        mlflow.log_param(\"learning_rate\", 1e-4)\n",
        "        mlflow.log_param(\"num_layers\", 12)\n",
        "        mlflow.log_param(\"embed_dim\", 768)\n",
        "        \n",
        "        print(f\"\\nðŸš€ Starting Performer Training with MLflow tracking...\")\n",
        "        print(f\"   Run name: Performer-{epochs}epochs\")\n",
        "        \n",
        "        for epoch in range(epochs):\n",
        "            model.train()\n",
        "            running_loss, correct, total = 0, 0, 0\n",
        "\n",
        "            for images, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs} [TRAIN]\"):\n",
        "                images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "                outputs = model(images)\n",
        "                loss = criterion(outputs, labels)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "                running_loss += loss.item()\n",
        "                _, preds = torch.max(outputs, 1)\n",
        "                correct += (preds == labels).sum().item()\n",
        "                total += labels.size(0)\n",
        "\n",
        "            train_acc = 100 * correct / total\n",
        "            train_loss = running_loss / len(train_loader)\n",
        "            \n",
        "            # Validation\n",
        "            val_acc = test_performer(model, val_loader, device)\n",
        "            \n",
        "            print(f\"Epoch [{epoch+1}/{epochs}] â†’ Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%, Val Acc: {val_acc:.2f}%\")\n",
        "            \n",
        "            # Log metrics to MLflow\n",
        "            mlflow.log_metric(\"train_loss\", train_loss, step=epoch)\n",
        "            mlflow.log_metric(\"train_accuracy\", train_acc, step=epoch)\n",
        "            mlflow.log_metric(\"val_accuracy\", val_acc, step=epoch)\n",
        "        \n",
        "        # SAVE AND LOG THE MODEL\n",
        "        print(f\"\\nðŸ’¾ Saving model to MLflow...\")\n",
        "        mlflow.pytorch.log_model(model, artifact_path=\"performer_model\")\n",
        "        \n",
        "        print(f\"âœ… MLflow run completed! View at http://localhost:5000\")\n",
        "        \n",
        "    return model\n",
        "\n",
        "# Initialize\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.AdamW(performer_model.parameters(), lr=1e-4)\n",
        "\n",
        "print(f\"\\nðŸ“Š Performer Model Setup:\")\n",
        "print(f\"   Device: {device}\")\n",
        "\n",
        "# Train with MLflow tracking\n",
        "performer_model = train_performer(performer_model, train_dataloader, val_dataloader, criterion, optimizer, device, epochs=5)\n",
        "\n",
        "# Final test\n",
        "performer_test_acc = test_performer(performer_model, test_dataloader, device)\n",
        "print(f\"\\nâœ… Final Performer Test Accuracy: {performer_test_acc:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p_hI7PM9TmgQ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NU1JdNztv9WB"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Va3BFeGQTmjn"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MLflow Tracking URI: file:///d:/Blood%20Cell%20Classifiaction/blood-cell-classification/notebooks/mlruns\n",
            "âœ“ Successfully connected to MLflow!\n"
          ]
        }
      ],
      "source": [
        "import mlflow\n",
        "\n",
        "# Test logging to verify connection\n",
        "print(f\"MLflow Tracking URI: {mlflow.get_tracking_uri()}\")\n",
        "with mlflow.start_run():\n",
        "    print(\"âœ“ Successfully connected to MLflow!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025/11/30 00:55:26 INFO mlflow.tracking.fluent: Experiment with name 'MLflow Quickstart' does not exist. Creating a new experiment.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<Experiment: artifact_location='file:///d:/Blood%20Cell%20Classifiaction/blood-cell-classification/notebooks/mlruns/386924970153409238', creation_time=1764444326364, experiment_id='386924970153409238', last_update_time=1764444326364, lifecycle_stage='active', name='MLflow Quickstart', tags={}>"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import mlflow\n",
        "\n",
        "mlflow.set_experiment(\"MLflow Quickstart\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting protobuf==3.20.3"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\sunil kumar\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\sunil kumar\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\sunil kumar\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n",
            "    WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\sunil kumar\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\sunil kumar\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n",
            "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "opentelemetry-proto 1.38.0 requires protobuf<7.0,>=5.0, but you have protobuf 3.20.3 which is incompatible.\n",
            "databricks-sdk 0.73.0 requires protobuf!=5.26.*,!=5.27.*,!=5.28.*,!=5.29.0,!=5.29.1,!=5.29.2,!=5.29.3,!=5.29.4,!=6.30.0,!=6.30.1,!=6.31.0,<7.0,>=4.25.8, but you have protobuf 3.20.3 which is incompatible.\n",
            "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\sunil kumar\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\sunil kumar\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\sunil kumar\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n",
            "WARNING: You are using pip version 21.2.3; however, version 25.3 is available.\n",
            "You should consider upgrading via the 'C:\\Users\\Sunil Kumar\\AppData\\Local\\Programs\\Python\\Python310\\python.exe -m pip install --upgrade pip' command.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "  Downloading protobuf-3.20.3-cp310-cp310-win_amd64.whl (904 kB)\n",
            "Installing collected packages: protobuf\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 6.33.1\n",
            "    Uninstalling protobuf-6.33.1:\n",
            "      Successfully uninstalled protobuf-6.33.1\n",
            "Successfully installed protobuf-3.20.3\n"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade protobuf==3.20.3\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tracking URI: file:///d:/Blood%20Cell%20Classifiaction/blood-cell-classification/notebooks/mlruns\n"
          ]
        }
      ],
      "source": [
        "import mlflow\n",
        "print(\"Tracking URI:\", mlflow.get_tracking_uri())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "================================================================================\n",
            "ðŸ“Š LOGGING TRAINED MODEL RESULTS TO MLFLOW\n",
            "================================================================================\n",
            "\n",
            "ðŸš€ Logging Custom ViT Model Results...\n",
            "âœ… Custom ViT results logged!\n",
            "\n",
            "ðŸš€ Logging PreTrained ViT Model Results...\n",
            "âœ… PreTrained ViT results logged!\n",
            "\n",
            "ðŸš€ Logging Performer Model Results...\n",
            "âœ… Performer results logged!\n",
            "\n",
            "================================================================================\n",
            "âœ… ALL RESULTS LOGGED TO MLFLOW!\n",
            "================================================================================\n",
            "\n",
            "ðŸ“Š SUMMARY:\n",
            "  â€¢ Custom ViT:        Final Test Accuracy = 84.74%\n",
            "  â€¢ PreTrained ViT:    Final Test Accuracy = 97.12% â­ BEST\n",
            "  â€¢ Performer Model:   Final Test Accuracy = 88.90%\n",
            "\n",
            "ðŸŒ Open MLflow Dashboard:\n",
            "   ðŸ‘‰ http://localhost:5000\n",
            "\n",
            "ðŸ“ Experiment: Blood-Cell-Classification-Results\n",
            "================================================================================\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import mlflow\n",
        "import mlflow.pytorch\n",
        "\n",
        "# Set MLflow tracking URI\n",
        "mlflow.set_tracking_uri(\"file:///D:/Blood Cell Classifiaction/blood-cell-classification/mlruns\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"ðŸ“Š LOGGING TRAINED MODEL RESULTS TO MLFLOW\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "mlflow.set_experiment(\"Blood-Cell-Classification-Results\")\n",
        "\n",
        "# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
        "# 1ï¸âƒ£ CUSTOM VIT MODEL RESULTS\n",
        "# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
        "print(\"\\nðŸš€ Logging Custom ViT Model Results...\")\n",
        "with mlflow.start_run(run_name=\"Custom-ViT-Model\"):\n",
        "    mlflow.log_param(\"model_type\", \"Custom Vision Transformer\")\n",
        "    mlflow.log_param(\"epochs\", 5)\n",
        "    mlflow.log_param(\"optimizer\", \"AdamW\")\n",
        "    mlflow.log_param(\"learning_rate\", 1e-4)\n",
        "    mlflow.log_param(\"batch_size\", 32)\n",
        "\n",
        "    # Epoch-wise metrics\n",
        "    loss_list = [1.4277, 0.8736, 0.6239, 0.7036, 0.5512]\n",
        "    train_acc_list = [28.13, 62.50, 72.64, 69.95, 75.38]\n",
        "    val_acc_list = [56.12, 83.21, 84.73, 86.89, 86.49]\n",
        "\n",
        "    for epoch in range(5):\n",
        "        mlflow.log_metric(\"loss\", loss_list[epoch], step=epoch+1)\n",
        "        mlflow.log_metric(\"train_accuracy\", train_acc_list[epoch], step=epoch+1)\n",
        "        mlflow.log_metric(\"val_accuracy\", val_acc_list[epoch], step=epoch+1)\n",
        "\n",
        "    mlflow.log_metric(\"final_test_accuracy\", 84.74)\n",
        "    print(\"âœ… Custom ViT results logged!\")\n",
        "\n",
        "# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
        "# 2ï¸âƒ£ PRETRAINED VIT MODEL RESULTS\n",
        "# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
        "print(\"\\nðŸš€ Logging PreTrained ViT Model Results...\")\n",
        "with mlflow.start_run(run_name=\"PreTrained-ViT-Model\"):\n",
        "    mlflow.log_param(\"model_type\", \"Vision Transformer (PreTrained ViT-B/16)\")\n",
        "    mlflow.log_param(\"epochs\", 5)\n",
        "    mlflow.log_param(\"optimizer\", \"AdamW\")\n",
        "    mlflow.log_param(\"learning_rate\", 1e-4)\n",
        "    mlflow.log_param(\"batch_size\", 32)\n",
        "    mlflow.log_param(\"pretrained_weights\", \"ViT_B_16_Weights.DEFAULT\")\n",
        "\n",
        "    # Epoch-wise metrics\n",
        "    loss_list = [0.5570, 0.3417, 0.3072, 0.2820, 0.2511]\n",
        "    train_acc_list = [75.55, 85.24, 87.06, 87.87, 89.58]\n",
        "    val_acc_list = [91.93, 96.56, 96.24, 96.64, 97.04]\n",
        "\n",
        "    for epoch in range(5):\n",
        "        mlflow.log_metric(\"loss\", loss_list[epoch], step=epoch+1)\n",
        "        mlflow.log_metric(\"train_accuracy\", train_acc_list[epoch], step=epoch+1)\n",
        "        mlflow.log_metric(\"val_accuracy\", val_acc_list[epoch], step=epoch+1)\n",
        "\n",
        "    mlflow.log_metric(\"final_test_accuracy\", 97.12)\n",
        "    print(\"âœ… PreTrained ViT results logged!\")\n",
        "\n",
        "# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
        "# 3ï¸âƒ£ PERFORMER MODEL RESULTS\n",
        "# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
        "print(\"\\nðŸš€ Logging Performer Model Results...\")\n",
        "with mlflow.start_run(run_name=\"Performer-Model\"):\n",
        "    mlflow.log_param(\"model_type\", \"Performer (Efficient Transformer)\")\n",
        "    mlflow.log_param(\"epochs\", 5)\n",
        "    mlflow.log_param(\"optimizer\", \"AdamW\")\n",
        "    mlflow.log_param(\"learning_rate\", 1e-4)\n",
        "    mlflow.log_param(\"batch_size\", 32)\n",
        "    mlflow.log_param(\"num_layers\", 12)\n",
        "    mlflow.log_param(\"embed_dim\", 768)\n",
        "    mlflow.log_param(\"num_heads\", 12)\n",
        "\n",
        "    # Epoch-wise metrics\n",
        "    loss_list = [1.4087, 0.9530, 0.6807, 0.5925, 0.5276]\n",
        "    train_acc_list = [30.07, 57.67, 70.94, 74.37, 76.98]\n",
        "    val_acc_list = [46.52, 77.30, 83.37, 87.77, 88.09]\n",
        "\n",
        "    for epoch in range(5):\n",
        "        mlflow.log_metric(\"loss\", loss_list[epoch], step=epoch+1)\n",
        "        mlflow.log_metric(\"train_accuracy\", train_acc_list[epoch], step=epoch+1)\n",
        "        mlflow.log_metric(\"val_accuracy\", val_acc_list[epoch], step=epoch+1)\n",
        "\n",
        "    mlflow.log_metric(\"final_test_accuracy\", 88.90)\n",
        "    print(\"âœ… Performer results logged!\")\n",
        "\n",
        "# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
        "# SUMMARY\n",
        "# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"âœ… ALL RESULTS LOGGED TO MLFLOW!\")\n",
        "print(\"=\"*80)\n",
        "print(\"\\nðŸ“Š SUMMARY:\")\n",
        "print(f\"  â€¢ Custom ViT:        Final Test Accuracy = 84.74%\")\n",
        "print(f\"  â€¢ PreTrained ViT:    Final Test Accuracy = 97.12% â­ BEST\")\n",
        "print(f\"  â€¢ Performer Model:   Final Test Accuracy = 88.90%\")\n",
        "print(\"\\nðŸŒ Open MLflow Dashboard:\")\n",
        "print(\"   ðŸ‘‰ http://localhost:5000\")\n",
        "print(\"\\nðŸ“ Experiment: Blood-Cell-Classification-Results\")\n",
        "print(\"=\"*80 + \"\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tracking URI: file:///D:/Blood Cell Classifiaction/blood-cell-classification/mlruns\n"
          ]
        }
      ],
      "source": [
        "import mlflow\n",
        "print(\"Tracking URI:\", mlflow.get_tracking_uri())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "import mlflow\n",
        "import mlflow.pytorch\n",
        "\n",
        "mlflow.set_tracking_uri(\"file:///D:/Blood Cell Classifiaction/blood-cell-classification/mlruns\")\n",
        "mlflow.set_experiment(\"Blood-Cell-Classification-Results\")\n",
        "\n",
        "def log_model_results(run_name, model_type, epochs, metrics, params=None):\n",
        "    \"\"\"Log metrics and parameters for a model to MLflow.\"\"\"\n",
        "    if params is None:\n",
        "        params = {}\n",
        "    \n",
        "    with mlflow.start_run(run_name=run_name):\n",
        "        # Log parameters\n",
        "        mlflow.log_param(\"model_type\", model_type)\n",
        "        mlflow.log_param(\"epochs\", epochs)\n",
        "        for key, val in params.items():\n",
        "            mlflow.log_param(key, val)\n",
        "        \n",
        "        # Log metrics per epoch\n",
        "        for metric_type in [\"train_acc\", \"val_acc\", \"loss\"]:\n",
        "            if metric_type in metrics:\n",
        "                for epoch in range(1, epochs + 1):\n",
        "                    key = f\"epoch_{epoch}_{metric_type}\"\n",
        "                    if key in metrics[metric_type]:\n",
        "                        mlflow.log_metric(metric_type, metrics[metric_type][key], step=epoch)\n",
        "        \n",
        "        # Log final test accuracy\n",
        "        if \"final_test_accuracy\" in metrics:\n",
        "            mlflow.log_metric(\"final_test_accuracy\", metrics[\"final_test_accuracy\"])\n",
        "\n",
        "# Example: Custom ViT\n",
        "custom_vit_metrics = {\n",
        "    \"train_acc\": {\n",
        "        \"epoch_1_train_acc\":28.13, \"epoch_2_train_acc\":62.50,\n",
        "        \"epoch_3_train_acc\":72.64, \"epoch_4_train_acc\":69.95,\n",
        "        \"epoch_5_train_acc\":75.38\n",
        "    },\n",
        "    \"val_acc\": {\n",
        "        \"epoch_1_val_acc\":56.12, \"epoch_2_val_acc\":83.21,\n",
        "        \"epoch_3_val_acc\":84.73, \"epoch_4_val_acc\":86.89,\n",
        "        \"epoch_5_val_acc\":86.49\n",
        "    },\n",
        "    \"loss\": {\n",
        "        \"epoch_1_loss\":1.4277, \"epoch_2_loss\":0.8736,\n",
        "        \"epoch_3_loss\":0.6239, \"epoch_4_loss\":0.7036,\n",
        "        \"epoch_5_loss\":0.5512\n",
        "    },\n",
        "    \"final_test_accuracy\": 84.74\n",
        "}\n",
        "custom_params = {\"optimizer\":\"AdamW\",\"learning_rate\":1e-4,\"batch_size\":32}\n",
        "\n",
        "log_model_results(\"Custom-ViT-Model\", \"Custom Vision Transformer\", 5, custom_vit_metrics, custom_params)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
