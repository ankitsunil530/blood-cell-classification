{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sU49HUdckIID",
        "outputId": "a1f8c525-3227-4ee0-b1db-2bc9e7c1332a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "fatal: destination path 'blood-cell-classification' already exists and is not an empty directory.\n",
            "/content/blood-cell-classification/blood-cell-classification\n"
          ]
        }
      ],
      "source": [
        "# !git clone https://github.com/ankitsunil530/blood-cell-classification.git\n",
        "# %cd blood-cell-classification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aiGH1t3MkQ8D",
        "outputId": "3119b7e3-ce71-43d5-fc1f-d12300e470d6"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'google.colab'",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgoogle\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcolab\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m drive\n\u001b[32m      2\u001b[39m drive.mount(\u001b[33m'\u001b[39m\u001b[33m/content/drive\u001b[39m\u001b[33m'\u001b[39m)\n",
            "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'google.colab'"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o77FxEgYMI7U"
      },
      "source": [
        "# **Blood Cell Classification**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c1bd6044"
      },
      "source": [
        "## Data loading and preprocessing\n",
        "\n",
        "### Subtask:\n",
        "Load the blood cell dataset and preprocess the images for use with the vision transformer and Performer models. This will involve resizing, normalization, and splitting the data into training, validation, and test sets.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "e819139e"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "caabfb6a"
      },
      "outputs": [],
      "source": [
        "DATASET_PATH = \"/content/drive/MyDrive/Deep Learning Lab/Blood_Cells_Dataset/Blood_Cells_Dataset\"\n",
        "IMG_HEIGHT = 224\n",
        "IMG_WIDTH = 224\n",
        "\n",
        "def load_images_and_labels(dataset_path):\n",
        "    images = []\n",
        "    labels = []\n",
        "    class_names = sorted(os.listdir(dataset_path))\n",
        "    class_to_idx = {class_name: i for i, class_name in enumerate(class_names)}\n",
        "\n",
        "    for class_name in class_names:\n",
        "        class_dir = os.path.join(dataset_path, class_name)\n",
        "        if os.path.isdir(class_dir):\n",
        "            for img_name in os.listdir(class_dir):\n",
        "                img_path = os.path.join(class_dir, img_name)\n",
        "                img = cv2.imread(img_path)\n",
        "                if img is not None:\n",
        "                    img = cv2.resize(img, (IMG_WIDTH, IMG_HEIGHT))\n",
        "                    images.append(img)\n",
        "                    labels.append(class_to_idx[class_name])\n",
        "\n",
        "    return np.array(images), np.array(labels), class_names\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5v91fwvHf3YN",
        "outputId": "50211248-6f50-496e-ad79-b335218cfc7a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Blood_Cells_Dataset/\n",
            "    dataset2-master/\n",
            "        dataset2-master/\n",
            "            images/\n",
            "                TEST_SIMPLE/\n",
            "                    MONOCYTE/\n",
            "                    NEUTROPHIL/\n",
            "                    LYMPHOCYTE/\n",
            "                    EOSINOPHIL/\n",
            "                TEST/\n",
            "                    NEUTROPHIL/\n",
            "                    MONOCYTE/\n",
            "                    LYMPHOCYTE/\n",
            "                    EOSINOPHIL/\n",
            "                TRAIN/\n",
            "                    NEUTROPHIL/\n",
            "                    MONOCYTE/\n",
            "                    LYMPHOCYTE/\n",
            "                    EOSINOPHIL/\n",
            "    dataset-master/\n",
            "        dataset-master/\n",
            "            Annotations/\n",
            "            JPEGImages/\n"
          ]
        }
      ],
      "source": [
        "for root, dirs, files in os.walk(DATASET_PATH):\n",
        "    level = root.replace(DATASET_PATH, '').count(os.sep)\n",
        "    indent = ' ' * 4 * (level)\n",
        "    print(f'{indent}{os.path.basename(root)}/')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c0b77a45",
        "outputId": "3107e169-b8e7-4831-c9dd-07ec7a07276f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found classes: ['EOSINOPHIL', 'LYMPHOCYTE', 'MONOCYTE', 'NEUTROPHIL']\n",
            "Processing directory: /content/drive/MyDrive/Deep Learning Lab/Blood_Cells_Dataset/Blood_Cells_Dataset/dataset2-master/dataset2-master/images/TRAIN/NEUTROPHIL\n",
            "Processing directory: /content/drive/MyDrive/Deep Learning Lab/Blood_Cells_Dataset/Blood_Cells_Dataset/dataset2-master/dataset2-master/images/TRAIN/MONOCYTE\n",
            "Processing directory: /content/drive/MyDrive/Deep Learning Lab/Blood_Cells_Dataset/Blood_Cells_Dataset/dataset2-master/dataset2-master/images/TRAIN/LYMPHOCYTE\n",
            "Processing directory: /content/drive/MyDrive/Deep Learning Lab/Blood_Cells_Dataset/Blood_Cells_Dataset/dataset2-master/dataset2-master/images/TRAIN/EOSINOPHIL\n",
            "Processing directory: /content/drive/MyDrive/Deep Learning Lab/Blood_Cells_Dataset/Blood_Cells_Dataset/dataset2-master/dataset2-master/images/TEST/NEUTROPHIL\n",
            "Processing directory: /content/drive/MyDrive/Deep Learning Lab/Blood_Cells_Dataset/Blood_Cells_Dataset/dataset2-master/dataset2-master/images/TEST/MONOCYTE\n",
            "Processing directory: /content/drive/MyDrive/Deep Learning Lab/Blood_Cells_Dataset/Blood_Cells_Dataset/dataset2-master/dataset2-master/images/TEST/LYMPHOCYTE\n",
            "Processing directory: /content/drive/MyDrive/Deep Learning Lab/Blood_Cells_Dataset/Blood_Cells_Dataset/dataset2-master/dataset2-master/images/TEST/EOSINOPHIL\n",
            "Processing directory: /content/drive/MyDrive/Deep Learning Lab/Blood_Cells_Dataset/Blood_Cells_Dataset/dataset2-master/dataset2-master/images/TEST_SIMPLE/MONOCYTE\n",
            "Processing directory: /content/drive/MyDrive/Deep Learning Lab/Blood_Cells_Dataset/Blood_Cells_Dataset/dataset2-master/dataset2-master/images/TEST_SIMPLE/NEUTROPHIL\n",
            "Processing directory: /content/drive/MyDrive/Deep Learning Lab/Blood_Cells_Dataset/Blood_Cells_Dataset/dataset2-master/dataset2-master/images/TEST_SIMPLE/LYMPHOCYTE\n",
            "Processing directory: /content/drive/MyDrive/Deep Learning Lab/Blood_Cells_Dataset/Blood_Cells_Dataset/dataset2-master/dataset2-master/images/TEST_SIMPLE/EOSINOPHIL\n",
            "Loaded 12515 images and 12515 labels.\n",
            "Training set size: 10012\n",
            "Validation set size: 1251\n",
            "Test set size: 1252\n",
            "Class names: ['EOSINOPHIL', 'LYMPHOCYTE', 'MONOCYTE', 'NEUTROPHIL']\n"
          ]
        }
      ],
      "source": [
        "def load_images_and_labels(dataset_path):\n",
        "    images = []\n",
        "    labels = []\n",
        "    # Update base directories to reflect the actual structure\n",
        "    base_image_dirs = [\n",
        "        os.path.join('dataset2-master', 'dataset2-master', 'images', 'TRAIN'),\n",
        "        os.path.join('dataset2-master', 'dataset2-master', 'images', 'TEST'),\n",
        "        os.path.join('dataset2-master', 'dataset2-master', 'images', 'TEST_SIMPLE')\n",
        "    ]\n",
        "    class_names = set()\n",
        "\n",
        "    # First pass to collect all unique class names\n",
        "    for base_img_dir_name in base_image_dirs:\n",
        "        base_img_dir_path = os.path.join(dataset_path, base_img_dir_name)\n",
        "        if os.path.isdir(base_img_dir_path):\n",
        "            for class_name in os.listdir(base_img_dir_path):\n",
        "                class_dir_path = os.path.join(base_img_dir_path, class_name)\n",
        "                if os.path.isdir(class_dir_path):\n",
        "                    class_names.add(class_name)\n",
        "\n",
        "    class_names = sorted(list(class_names))\n",
        "    class_to_idx = {class_name: i for i, class_name in enumerate(class_names)}\n",
        "\n",
        "    print(f\"Found classes: {class_names}\")\n",
        "\n",
        "    # Second pass to load images\n",
        "    for base_img_dir_name in base_image_dirs:\n",
        "        base_img_dir_path = os.path.join(dataset_path, base_img_dir_name)\n",
        "        if os.path.isdir(base_img_dir_path):\n",
        "            for class_name in os.listdir(base_img_dir_path):\n",
        "                class_dir_path = os.path.join(base_img_dir_path, class_name)\n",
        "                print(f\"Processing directory: {class_dir_path}\")\n",
        "                if os.path.isdir(class_dir_path) and class_name in class_to_idx:\n",
        "                    for img_name in os.listdir(class_dir_path):\n",
        "                        img_path = os.path.join(class_dir_path, img_name)\n",
        "                        if os.path.isfile(img_path) and img_name.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
        "                            img = cv2.imread(img_path)\n",
        "                            if img is not None:\n",
        "                                img = cv2.resize(img, (IMG_WIDTH, IMG_HEIGHT))\n",
        "                                images.append(img)\n",
        "                                labels.append(class_to_idx[class_name])\n",
        "                            else:\n",
        "                                print(f\"Could not load image: {img_path}\")\n",
        "\n",
        "    print(f\"Loaded {len(images)} images and {len(labels)} labels.\")\n",
        "    return np.array(images), np.array(labels), class_names\n",
        "\n",
        "images, labels, class_names = load_images_and_labels(DATASET_PATH)\n",
        "\n",
        "# Split the data - combining images from TRAIN, TEST, TEST_SIMPLE before splitting\n",
        "# Using a fixed random state for reproducibility\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(images, labels, test_size=0.2, stratify=labels, random_state=42)\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, stratify=y_temp, random_state=42)\n",
        "\n",
        "print(f\"Training set size: {len(X_train)}\")\n",
        "print(f\"Validation set size: {len(X_val)}\")\n",
        "print(f\"Test set size: {len(X_test)}\")\n",
        "print(f\"Class names: {class_names}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d77d6a6a"
      },
      "source": [
        "## Data loading and preprocessing\n",
        "\n",
        "### Subtask:\n",
        "Load the blood cell dataset and preprocess the images for use with the vision transformer and Performer models. This will involve resizing, normalization, and splitting the data into training, validation, and test sets.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "96d53a02"
      },
      "outputs": [],
      "source": [
        "class BloodCellDataset(Dataset):\n",
        "    def __init__(self, images, labels, transform=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            images (numpy array): Array of image data (H, W, C).\n",
        "            labels (numpy array): Array of corresponding labels.\n",
        "            transform (callable, optional): Optional transform to be applied\n",
        "                on a sample.\n",
        "        \"\"\"\n",
        "        self.images = images\n",
        "        self.labels = labels\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if torch.is_tensor(idx):\n",
        "            idx = idx.tolist()\n",
        "\n",
        "        image = self.images[idx]\n",
        "        label = self.labels[idx]\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, label\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "25837724",
        "outputId": "3f5858ea-30d4-49e7-b7f4-346399aceee2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of training batches: 313\n",
            "Number of validation batches: 40\n",
            "Number of test batches: 40\n"
          ]
        }
      ],
      "source": [
        "# Define transformations\n",
        "# Using common mean and std dev for ImageNet as a starting point\n",
        "# A more accurate approach would be to calculate these from the dataset\n",
        "mean = [0.485, 0.456, 0.406]\n",
        "std = [0.229, 0.224, 0.225]\n",
        "\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.ToPILImage(), # Convert numpy array to PIL Image for transforms\n",
        "    transforms.RandomResizedCrop(IMG_HEIGHT),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=mean, std=std),\n",
        "])\n",
        "\n",
        "val_test_transform = transforms.Compose([\n",
        "    transforms.ToPILImage(), # Convert numpy array to PIL Image for transforms\n",
        "    transforms.Resize(IMG_HEIGHT),\n",
        "    transforms.CenterCrop(IMG_HEIGHT),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=mean, std=std),\n",
        "])\n",
        "\n",
        "# Create Dataset instances\n",
        "train_dataset = BloodCellDataset(X_train, y_train, transform=train_transform)\n",
        "val_dataset = BloodCellDataset(X_val, y_val, transform=val_test_transform)\n",
        "test_dataset = BloodCellDataset(X_test, y_test, transform=val_test_transform)\n",
        "\n",
        "# Create DataLoader instances\n",
        "batch_size = 32 # Using the already defined batch_size\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "print(f\"Number of training batches: {len(train_dataloader)}\")\n",
        "print(f\"Number of validation batches: {len(val_dataloader)}\")\n",
        "print(f\"Number of test batches: {len(test_dataloader)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7757e7a6"
      },
      "source": [
        "## Vision transformer model implementation\n",
        "\n",
        "### Subtask:\n",
        "Implement the Vision Transformer model for blood cell classification.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "59d19bcc"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "from torchvision.models import vit_b_16, ViT_B_16_Weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7cae6304",
        "outputId": "9b62c8e1-8c62-4646-ce42-d544cfe90012"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model initialized and moved to cuda\n",
            "BloodCellViT(\n",
            "  (vit): VisionTransformer(\n",
            "    (conv_proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
            "    (encoder): Encoder(\n",
            "      (dropout): Dropout(p=0.0, inplace=False)\n",
            "      (layers): Sequential(\n",
            "        (encoder_layer_0): EncoderBlock(\n",
            "          (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "          (self_attention): MultiheadAttention(\n",
            "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
            "          )\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "          (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "          (mlp): MLPBlock(\n",
            "            (0): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (1): GELU(approximate='none')\n",
            "            (2): Dropout(p=0.0, inplace=False)\n",
            "            (3): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (4): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (encoder_layer_1): EncoderBlock(\n",
            "          (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "          (self_attention): MultiheadAttention(\n",
            "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
            "          )\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "          (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "          (mlp): MLPBlock(\n",
            "            (0): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (1): GELU(approximate='none')\n",
            "            (2): Dropout(p=0.0, inplace=False)\n",
            "            (3): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (4): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (encoder_layer_2): EncoderBlock(\n",
            "          (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "          (self_attention): MultiheadAttention(\n",
            "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
            "          )\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "          (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "          (mlp): MLPBlock(\n",
            "            (0): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (1): GELU(approximate='none')\n",
            "            (2): Dropout(p=0.0, inplace=False)\n",
            "            (3): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (4): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (encoder_layer_3): EncoderBlock(\n",
            "          (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "          (self_attention): MultiheadAttention(\n",
            "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
            "          )\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "          (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "          (mlp): MLPBlock(\n",
            "            (0): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (1): GELU(approximate='none')\n",
            "            (2): Dropout(p=0.0, inplace=False)\n",
            "            (3): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (4): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (encoder_layer_4): EncoderBlock(\n",
            "          (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "          (self_attention): MultiheadAttention(\n",
            "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
            "          )\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "          (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "          (mlp): MLPBlock(\n",
            "            (0): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (1): GELU(approximate='none')\n",
            "            (2): Dropout(p=0.0, inplace=False)\n",
            "            (3): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (4): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (encoder_layer_5): EncoderBlock(\n",
            "          (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "          (self_attention): MultiheadAttention(\n",
            "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
            "          )\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "          (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "          (mlp): MLPBlock(\n",
            "            (0): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (1): GELU(approximate='none')\n",
            "            (2): Dropout(p=0.0, inplace=False)\n",
            "            (3): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (4): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (encoder_layer_6): EncoderBlock(\n",
            "          (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "          (self_attention): MultiheadAttention(\n",
            "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
            "          )\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "          (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "          (mlp): MLPBlock(\n",
            "            (0): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (1): GELU(approximate='none')\n",
            "            (2): Dropout(p=0.0, inplace=False)\n",
            "            (3): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (4): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (encoder_layer_7): EncoderBlock(\n",
            "          (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "          (self_attention): MultiheadAttention(\n",
            "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
            "          )\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "          (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "          (mlp): MLPBlock(\n",
            "            (0): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (1): GELU(approximate='none')\n",
            "            (2): Dropout(p=0.0, inplace=False)\n",
            "            (3): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (4): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (encoder_layer_8): EncoderBlock(\n",
            "          (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "          (self_attention): MultiheadAttention(\n",
            "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
            "          )\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "          (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "          (mlp): MLPBlock(\n",
            "            (0): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (1): GELU(approximate='none')\n",
            "            (2): Dropout(p=0.0, inplace=False)\n",
            "            (3): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (4): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (encoder_layer_9): EncoderBlock(\n",
            "          (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "          (self_attention): MultiheadAttention(\n",
            "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
            "          )\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "          (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "          (mlp): MLPBlock(\n",
            "            (0): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (1): GELU(approximate='none')\n",
            "            (2): Dropout(p=0.0, inplace=False)\n",
            "            (3): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (4): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (encoder_layer_10): EncoderBlock(\n",
            "          (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "          (self_attention): MultiheadAttention(\n",
            "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
            "          )\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "          (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "          (mlp): MLPBlock(\n",
            "            (0): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (1): GELU(approximate='none')\n",
            "            (2): Dropout(p=0.0, inplace=False)\n",
            "            (3): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (4): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (encoder_layer_11): EncoderBlock(\n",
            "          (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "          (self_attention): MultiheadAttention(\n",
            "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
            "          )\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "          (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "          (mlp): MLPBlock(\n",
            "            (0): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (1): GELU(approximate='none')\n",
            "            (2): Dropout(p=0.0, inplace=False)\n",
            "            (3): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (4): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (ln): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "    )\n",
            "    (heads): Sequential(\n",
            "      (head): Linear(in_features=768, out_features=4, bias=True)\n",
            "    )\n",
            "  )\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "class BloodCellViT(nn.Module):\n",
        "    def __init__(self, num_classes=4):\n",
        "        super(BloodCellViT, self).__init__()\n",
        "        # Load pre-trained ViT-Base/16\n",
        "        weights = ViT_B_16_Weights.DEFAULT\n",
        "        self.vit = vit_b_16(weights=None)\n",
        "        num_ftrs = self.vit.heads.head.in_features\n",
        "        self.vit.heads.head = nn.Linear(num_ftrs, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.vit(x)\n",
        "\n",
        "\n",
        "num_classes = 4 # Based on the previous data loading step\n",
        "model = BloodCellViT(num_classes=num_classes)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "print(f\"Model initialized and moved to {device}\")\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cf857af2"
      },
      "source": [
        "## Performer model implementation\n",
        "\n",
        "### Subtask:\n",
        "Implement the Performer model for blood cell classification.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "71e4448e"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "def linear_attention(q, k, v):\n",
        "    attention_weights = torch.matmul(q, k.transpose(-2, -1))\n",
        "    attention_weights = F.softmax(attention_weights, dim=-1)\n",
        "    output = torch.matmul(attention_weights, v)\n",
        "    return output\n",
        "\n",
        "class PatchEmbedding(nn.Module):\n",
        "    def __init__(self, img_size=224, patch_size=16, in_channels=3, embed_dim=768):\n",
        "        super().__init__()\n",
        "        self.img_size = img_size\n",
        "        self.patch_size = patch_size\n",
        "        self.num_patches = (img_size // patch_size) ** 2\n",
        "        self.proj = nn.Conv2d(in_channels, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.proj(x)\n",
        "        x = x.flatten(2)\n",
        "        x = x.transpose(1, 2)\n",
        "        return x\n",
        "\n",
        "class PerformerBlock(nn.Module):\n",
        "    def __init__(self, embed_dim, num_heads, mlp_ratio=4., drop_rate=0., attn_drop_rate=0.):\n",
        "        super().__init__()\n",
        "        self.norm1 = nn.LayerNorm(embed_dim)\n",
        "        self.attn = nn.MultiheadAttention(embed_dim, num_heads, dropout=attn_drop_rate, batch_first=True) # Using standard MultiheadAttention for now, would replace with efficient attention\n",
        "        self.norm2 = nn.LayerNorm(embed_dim)\n",
        "        mlp_hidden_dim = int(embed_dim * mlp_ratio)\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(embed_dim, mlp_hidden_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(drop_rate),\n",
        "            nn.Linear(mlp_hidden_dim, embed_dim),\n",
        "            nn.Dropout(drop_rate)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.attn(self.norm1(x), self.norm1(x), self.norm1(x))[0]\n",
        "        x = x + self.mlp(self.norm2(x))\n",
        "        return x\n",
        "\n",
        "class PerformerModel(nn.Module):\n",
        "    def __init__(self, img_size=224, patch_size=16, in_channels=3, embed_dim=768, num_layers=12, num_heads=12, mlp_ratio=4., num_classes=4, drop_rate=0., attn_drop_rate=0.):\n",
        "        super().__init__()\n",
        "        self.num_classes = num_classes\n",
        "        self.embed_dim = embed_dim\n",
        "\n",
        "        self.patch_embed = PatchEmbedding(img_size, patch_size, in_channels, embed_dim)\n",
        "        num_patches = self.patch_embed.num_patches\n",
        "\n",
        "        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches + 1, embed_dim))\n",
        "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
        "        self.pos_drop = nn.Dropout(p=drop_rate)\n",
        "\n",
        "        self.blocks = nn.ModuleList([\n",
        "            PerformerBlock(embed_dim, num_heads, mlp_ratio, drop_rate, attn_drop_rate)\n",
        "            for _ in range(num_layers)\n",
        "        ])\n",
        "\n",
        "        self.norm = nn.LayerNorm(embed_dim)\n",
        "        self.head = nn.Linear(embed_dim, num_classes) if num_classes > 0 else nn.Identity()\n",
        "\n",
        "        # Initialize weights\n",
        "        nn.init.trunc_normal_(self.pos_embed, std=.02)\n",
        "        nn.init.trunc_normal_(self.cls_token, std=.02)\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, m):\n",
        "        if isinstance(m, nn.Linear):\n",
        "            nn.init.trunc_normal_(m.weight, std=.02)\n",
        "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "        elif isinstance(m, nn.LayerNorm):\n",
        "            nn.init.constant_(m.bias, 0)\n",
        "            nn.init.constant_(m.weight, 1.0)\n",
        "\n",
        "    def forward_features(self, x):\n",
        "        B = x.shape[0]\n",
        "        x = self.patch_embed(x)\n",
        "\n",
        "        cls_tokens = self.cls_token.expand(B, -1, -1)\n",
        "        x = torch.cat((cls_tokens, x), dim=1)\n",
        "\n",
        "        x = x + self.pos_embed\n",
        "        x = self.pos_drop(x)\n",
        "\n",
        "        for blk in self.blocks:\n",
        "            x = blk(x)\n",
        "\n",
        "        x = self.norm(x)\n",
        "        return x[:, 0]\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.forward_features(x)\n",
        "        x = self.head(x)\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0a321914",
        "outputId": "4381058b-dd34-4c44-e23b-500690b67d5d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Performer Model initialized and moved to cuda\n",
            "PerformerModel(\n",
            "  (patch_embed): PatchEmbedding(\n",
            "    (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
            "  )\n",
            "  (pos_drop): Dropout(p=0.0, inplace=False)\n",
            "  (blocks): ModuleList(\n",
            "    (0-11): 12 x PerformerBlock(\n",
            "      (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "      (attn): MultiheadAttention(\n",
            "        (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
            "      )\n",
            "      (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "      (mlp): Sequential(\n",
            "        (0): Linear(in_features=768, out_features=3072, bias=True)\n",
            "        (1): GELU(approximate='none')\n",
            "        (2): Dropout(p=0.0, inplace=False)\n",
            "        (3): Linear(in_features=3072, out_features=768, bias=True)\n",
            "        (4): Dropout(p=0.0, inplace=False)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "  (head): Linear(in_features=768, out_features=4, bias=True)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "\n",
        "img_size = IMG_HEIGHT\n",
        "patch_size = 16\n",
        "in_channels = 3\n",
        "embed_dim = 768\n",
        "num_layers = 12\n",
        "num_heads = 12\n",
        "mlp_ratio = 4\n",
        "num_classes = num_classes\n",
        "\n",
        "performer_model = PerformerModel(\n",
        "    img_size=img_size,\n",
        "    patch_size=patch_size,\n",
        "    in_channels=in_channels,\n",
        "    embed_dim=embed_dim,\n",
        "    num_layers=num_layers,\n",
        "    num_heads=num_heads,\n",
        "    mlp_ratio=mlp_ratio,\n",
        "    num_classes=num_classes\n",
        ")\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "performer_model.to(device)\n",
        "\n",
        "print(f\"Performer Model initialized and moved to {device}\")\n",
        "print(performer_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e25d02f9"
      },
      "source": [
        "## Model training\n",
        "\n",
        "### Subtask:\n",
        "Train both the Vision Transformer and Performer models on the training data.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XkHFKZhWvaOR"
      },
      "source": [
        "# Newly Initialized ViT Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XzurHPcRpaif",
        "outputId": "a7f67a71-8636-49b8-cc40-519b54217b77"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1/5: 100%|██████████| 313/313 [06:24<00:00,  1.23s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/5] → Loss: 1.4277, Train Acc: 28.13%\n",
            "Validation Accuracy: 56.12%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2/5: 100%|██████████| 313/313 [06:24<00:00,  1.23s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [2/5] → Loss: 0.8736, Train Acc: 62.50%\n",
            "Validation Accuracy: 83.21%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 3/5: 100%|██████████| 313/313 [06:24<00:00,  1.23s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [3/5] → Loss: 0.6239, Train Acc: 72.64%\n",
            "Validation Accuracy: 84.73%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 4/5: 100%|██████████| 313/313 [06:25<00:00,  1.23s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [4/5] → Loss: 0.7036, Train Acc: 69.95%\n",
            "Validation Accuracy: 86.89%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 5/5: 100%|██████████| 313/313 [06:28<00:00,  1.24s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [5/5] → Loss: 0.5512, Train Acc: 75.38%\n",
            "Validation Accuracy: 86.49%\n",
            "\n",
            "✅ Final ViT Test Accuracy: 84.74%\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision.models import vit_b_16, ViT_B_16_Weights\n",
        "from tqdm import tqdm\n",
        "\n",
        "def train_vit(model, train_loader, val_loader, criterion, optimizer, device, epochs=5):\n",
        "    model.to(device)\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        running_loss, correct, total = 0, 0, 0\n",
        "\n",
        "        for images, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\"):\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            correct += (preds == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "\n",
        "        train_acc = 100 * correct / total\n",
        "        print(f\"Epoch [{epoch+1}/{epochs}] → Loss: {running_loss/len(train_loader):.4f}, Train Acc: {train_acc:.2f}%\")\n",
        "\n",
        "        val_acc = test_vit(model, val_loader, device)\n",
        "        print(f\"Validation Accuracy: {val_acc:.2f}%\")\n",
        "\n",
        "    return model\n",
        "\n",
        "def test_vit(model, test_loader, device):\n",
        "    model.eval()\n",
        "    correct, total = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in test_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            correct += (preds == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "    return 100 * correct / total\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "num_classes = 4\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "vit_model = BloodCellViT(num_classes)\n",
        "optimizer = optim.AdamW(vit_model.parameters(), lr=1e-4)\n",
        "\n",
        "vit_model = train_vit(vit_model, train_dataloader, val_dataloader, criterion, optimizer, device, epochs=5)\n",
        "vit_test_acc = test_vit(vit_model, test_dataloader, device)\n",
        "print(f\"\\n✅ Final ViT Test Accuracy: {vit_test_acc:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PGfmoVuWvJuK"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-LvukEj9vK9c"
      },
      "source": [
        "# **PreTrained ViT Model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iETPbvLPvFUe",
        "outputId": "c7e47edb-97a5-4716-9ae4-428b750766aa"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1/5: 100%|██████████| 313/313 [06:29<00:00,  1.24s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/5] → Loss: 0.5570, Train Acc: 75.55%\n",
            "Validation Accuracy: 91.93%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2/5: 100%|██████████| 313/313 [06:27<00:00,  1.24s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [2/5] → Loss: 0.3417, Train Acc: 85.24%\n",
            "Validation Accuracy: 96.56%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 3/5: 100%|██████████| 313/313 [06:28<00:00,  1.24s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [3/5] → Loss: 0.3072, Train Acc: 87.06%\n",
            "Validation Accuracy: 96.24%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 4/5: 100%|██████████| 313/313 [06:27<00:00,  1.24s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [4/5] → Loss: 0.2820, Train Acc: 87.87%\n",
            "Validation Accuracy: 96.64%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 5/5: 100%|██████████| 313/313 [06:28<00:00,  1.24s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [5/5] → Loss: 0.2511, Train Acc: 89.58%\n",
            "Validation Accuracy: 97.04%\n",
            "\n",
            "✅ Final ViT Test Accuracy: 97.12%\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision.models import vit_b_16, ViT_B_16_Weights\n",
        "from tqdm import tqdm\n",
        "import mlflow\n",
        "import mlflow.pytorch\n",
        "\n",
        "class BloodCellViT(nn.Module):\n",
        "    def __init__(self, num_classes=4):\n",
        "        super(BloodCellViT, self).__init__()\n",
        "        # Pretrained ViT model\n",
        "        weights = ViT_B_16_Weights.DEFAULT\n",
        "        self.vit = vit_b_16(weights=weights)\n",
        "        num_ftrs = self.vit.heads.head.in_features\n",
        "        self.vit.heads.head = nn.Linear(num_ftrs, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.vit(x)\n",
        "\n",
        "def test_vit(model, test_loader, device):\n",
        "    model.eval()\n",
        "    correct, total = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in test_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            correct += (preds == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "    return 100 * correct / total\n",
        "\n",
        "def train_vit(model, train_loader, val_loader, criterion, optimizer, device, epochs=5):\n",
        "    model.to(device)\n",
        "    \n",
        "    # START MLFLOW RUN\n",
        "    with mlflow.start_run(run_name=f\"ViT-PreTrained-{epochs}epochs\"):\n",
        "        \n",
        "        # Log hyperparameters\n",
        "        mlflow.log_param(\"model_type\", \"Vision Transformer (PreTrained)\")\n",
        "        mlflow.log_param(\"epochs\", epochs)\n",
        "        mlflow.log_param(\"optimizer\", \"AdamW\")\n",
        "        mlflow.log_param(\"batch_size\", train_loader.batch_size)\n",
        "        mlflow.log_param(\"device\", str(device))\n",
        "        mlflow.log_param(\"learning_rate\", 1e-4)\n",
        "        \n",
        "        print(f\"\\n🚀 Starting ViT Training with MLflow tracking...\")\n",
        "        print(f\"   Run name: ViT-PreTrained-{epochs}epochs\")\n",
        "        \n",
        "        for epoch in range(epochs):\n",
        "            model.train()\n",
        "            running_loss, correct, total = 0, 0, 0\n",
        "\n",
        "            for images, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs} [TRAIN]\"):\n",
        "                images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "                outputs = model(images)\n",
        "                loss = criterion(outputs, labels)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "                running_loss += loss.item()\n",
        "                _, preds = torch.max(outputs, 1)\n",
        "                correct += (preds == labels).sum().item()\n",
        "                total += labels.size(0)\n",
        "\n",
        "            train_acc = 100 * correct / total\n",
        "            train_loss = running_loss / len(train_loader)\n",
        "            \n",
        "            # Validation\n",
        "            val_acc = test_vit(model, val_loader, device)\n",
        "            \n",
        "            print(f\"Epoch [{epoch+1}/{epochs}] → Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%, Val Acc: {val_acc:.2f}%\")\n",
        "            \n",
        "            # Log metrics to MLflow\n",
        "            mlflow.log_metric(\"train_loss\", train_loss, step=epoch)\n",
        "            mlflow.log_metric(\"train_accuracy\", train_acc, step=epoch)\n",
        "            mlflow.log_metric(\"val_accuracy\", val_acc, step=epoch)\n",
        "        \n",
        "        # SAVE AND LOG THE MODEL\n",
        "        print(f\"\\n💾 Saving model to MLflow...\")\n",
        "        mlflow.pytorch.log_model(model, artifact_path=\"vit_model\")\n",
        "        \n",
        "        print(f\"✅ MLflow run completed! View at http://localhost:5000\")\n",
        "        \n",
        "    return model\n",
        "\n",
        "# Initialize\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "num_classes = 4\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "print(f\"\\n📊 Model Setup:\")\n",
        "print(f\"   Device: {device}\")\n",
        "print(f\"   Classes: {num_classes}\")\n",
        "\n",
        "vit_model = BloodCellViT(num_classes)\n",
        "optimizer = optim.AdamW(vit_model.parameters(), lr=1e-4)\n",
        "\n",
        "# Train with MLflow tracking\n",
        "vit_model = train_vit(vit_model, train_dataloader, val_dataloader, criterion, optimizer, device, epochs=5)\n",
        "\n",
        "# Final test\n",
        "vit_test_acc = test_vit(vit_model, test_dataloader, device)\n",
        "print(f\"\\n✅ Final ViT Test Accuracy: {vit_test_acc:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BZ7d8lmATmL5",
        "outputId": "08acdace-1299-4f86-bc65-b9bb5bcd3446"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1/5: 100%|██████████| 313/313 [06:19<00:00,  1.21s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/5] → Loss: 1.4087, Train Acc: 30.07%\n",
            "Validation Accuracy: 46.52%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2/5: 100%|██████████| 313/313 [06:19<00:00,  1.21s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [2/5] → Loss: 0.9530, Train Acc: 57.67%\n",
            "Validation Accuracy: 77.30%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 3/5: 100%|██████████| 313/313 [06:19<00:00,  1.21s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [3/5] → Loss: 0.6807, Train Acc: 70.94%\n",
            "Validation Accuracy: 83.37%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 4/5: 100%|██████████| 313/313 [06:19<00:00,  1.21s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [4/5] → Loss: 0.5925, Train Acc: 74.37%\n",
            "Validation Accuracy: 87.77%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 5/5: 100%|██████████| 313/313 [06:21<00:00,  1.22s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [5/5] → Loss: 0.5276, Train Acc: 76.98%\n",
            "Validation Accuracy: 88.09%\n",
            "\n",
            "✅ Final Performer Test Accuracy: 88.90%\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from tqdm import tqdm\n",
        "import mlflow\n",
        "import mlflow.pytorch\n",
        "\n",
        "def test_performer(model, test_loader, device):\n",
        "    model.eval()\n",
        "    correct, total = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in test_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            correct += (preds == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "    return 100 * correct / total\n",
        "\n",
        "def train_performer(model, train_loader, val_loader, criterion, optimizer, device, epochs=5):\n",
        "    model.to(device)\n",
        "    \n",
        "    # START MLFLOW RUN\n",
        "    with mlflow.start_run(run_name=f\"Performer-{epochs}epochs\"):\n",
        "        \n",
        "        # Log hyperparameters\n",
        "        mlflow.log_param(\"model_type\", \"Performer\")\n",
        "        mlflow.log_param(\"epochs\", epochs)\n",
        "        mlflow.log_param(\"optimizer\", \"AdamW\")\n",
        "        mlflow.log_param(\"batch_size\", train_loader.batch_size)\n",
        "        mlflow.log_param(\"device\", str(device))\n",
        "        mlflow.log_param(\"learning_rate\", 1e-4)\n",
        "        mlflow.log_param(\"num_layers\", 12)\n",
        "        mlflow.log_param(\"embed_dim\", 768)\n",
        "        \n",
        "        print(f\"\\n🚀 Starting Performer Training with MLflow tracking...\")\n",
        "        print(f\"   Run name: Performer-{epochs}epochs\")\n",
        "        \n",
        "        for epoch in range(epochs):\n",
        "            model.train()\n",
        "            running_loss, correct, total = 0, 0, 0\n",
        "\n",
        "            for images, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs} [TRAIN]\"):\n",
        "                images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "                outputs = model(images)\n",
        "                loss = criterion(outputs, labels)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "                running_loss += loss.item()\n",
        "                _, preds = torch.max(outputs, 1)\n",
        "                correct += (preds == labels).sum().item()\n",
        "                total += labels.size(0)\n",
        "\n",
        "            train_acc = 100 * correct / total\n",
        "            train_loss = running_loss / len(train_loader)\n",
        "            \n",
        "            # Validation\n",
        "            val_acc = test_performer(model, val_loader, device)\n",
        "            \n",
        "            print(f\"Epoch [{epoch+1}/{epochs}] → Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%, Val Acc: {val_acc:.2f}%\")\n",
        "            \n",
        "            # Log metrics to MLflow\n",
        "            mlflow.log_metric(\"train_loss\", train_loss, step=epoch)\n",
        "            mlflow.log_metric(\"train_accuracy\", train_acc, step=epoch)\n",
        "            mlflow.log_metric(\"val_accuracy\", val_acc, step=epoch)\n",
        "        \n",
        "        # SAVE AND LOG THE MODEL\n",
        "        print(f\"\\n💾 Saving model to MLflow...\")\n",
        "        mlflow.pytorch.log_model(model, artifact_path=\"performer_model\")\n",
        "        \n",
        "        print(f\"✅ MLflow run completed! View at http://localhost:5000\")\n",
        "        \n",
        "    return model\n",
        "\n",
        "# Initialize\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.AdamW(performer_model.parameters(), lr=1e-4)\n",
        "\n",
        "print(f\"\\n📊 Performer Model Setup:\")\n",
        "print(f\"   Device: {device}\")\n",
        "\n",
        "# Train with MLflow tracking\n",
        "performer_model = train_performer(performer_model, train_dataloader, val_dataloader, criterion, optimizer, device, epochs=5)\n",
        "\n",
        "# Final test\n",
        "performer_test_acc = test_performer(performer_model, test_dataloader, device)\n",
        "print(f\"\\n✅ Final Performer Test Accuracy: {performer_test_acc:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p_hI7PM9TmgQ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NU1JdNztv9WB"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Va3BFeGQTmjn"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "================================================================================\n",
            "📊 LOGGING TRAINED MODEL RESULTS TO MLFLOW\n",
            "================================================================================\n",
            "\n",
            "🚀 Logging Custom ViT Model Results...\n",
            "✅ Custom ViT results logged!\n",
            "\n",
            "🚀 Logging PreTrained ViT Model Results...\n",
            "✅ Custom ViT results logged!\n",
            "\n",
            "🚀 Logging PreTrained ViT Model Results...\n",
            "✅ PreTrained ViT results logged!\n",
            "\n",
            "🚀 Logging Performer Model Results...\n",
            "✅ PreTrained ViT results logged!\n",
            "\n",
            "🚀 Logging Performer Model Results...\n",
            "✅ Performer results logged!\n",
            "\n",
            "================================================================================\n",
            "✅ ALL RESULTS LOGGED TO MLFLOW!\n",
            "================================================================================\n",
            "\n",
            "📊 SUMMARY:\n",
            "  • Custom ViT:        Final Test Accuracy = 84.74%\n",
            "  • PreTrained ViT:    Final Test Accuracy = 97.12% ⭐ BEST\n",
            "  • Performer Model:   Final Test Accuracy = 88.90%\n",
            "\n",
            "🌐 Open MLflow Dashboard:\n",
            "   👉 http://localhost:5000\n",
            "\n",
            "📁 Experiment: Blood-Cell-Classification-Results\n",
            "================================================================================\n",
            "\n",
            "✅ Performer results logged!\n",
            "\n",
            "================================================================================\n",
            "✅ ALL RESULTS LOGGED TO MLFLOW!\n",
            "================================================================================\n",
            "\n",
            "📊 SUMMARY:\n",
            "  • Custom ViT:        Final Test Accuracy = 84.74%\n",
            "  • PreTrained ViT:    Final Test Accuracy = 97.12% ⭐ BEST\n",
            "  • Performer Model:   Final Test Accuracy = 88.90%\n",
            "\n",
            "🌐 Open MLflow Dashboard:\n",
            "   👉 http://localhost:5000\n",
            "\n",
            "📁 Experiment: Blood-Cell-Classification-Results\n",
            "================================================================================\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import mlflow\n",
        "import mlflow.pytorch\n",
        "\n",
        "# ✅ LOG EXISTING TRAINED RESULTS TO MLFLOW (Without Re-training)\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"📊 LOGGING TRAINED MODEL RESULTS TO MLFLOW\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "mlflow.set_experiment(\"Blood-Cell-Classification-Results\")\n",
        "\n",
        "# ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n",
        "# 1️⃣ CUSTOM VIT MODEL RESULTS\n",
        "# ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n",
        "\n",
        "print(\"\\n🚀 Logging Custom ViT Model Results...\")\n",
        "with mlflow.start_run(run_name=\"Custom-ViT-Model\"):\n",
        "    mlflow.log_param(\"model_type\", \"Custom Vision Transformer\")\n",
        "    mlflow.log_param(\"epochs\", 5)\n",
        "    mlflow.log_param(\"optimizer\", \"AdamW\")\n",
        "    mlflow.log_param(\"learning_rate\", 1e-4)\n",
        "    mlflow.log_param(\"batch_size\", 32)\n",
        "    \n",
        "    # Custom ViT Results\n",
        "    mlflow.log_metric(\"final_test_accuracy\", 84.74)\n",
        "    mlflow.log_metric(\"epoch_1_loss\", 1.4277)\n",
        "    mlflow.log_metric(\"epoch_1_train_acc\", 28.13)\n",
        "    mlflow.log_metric(\"epoch_1_val_acc\", 56.12)\n",
        "    mlflow.log_metric(\"epoch_2_loss\", 0.8736)\n",
        "    mlflow.log_metric(\"epoch_2_train_acc\", 62.50)\n",
        "    mlflow.log_metric(\"epoch_2_val_acc\", 83.21)\n",
        "    mlflow.log_metric(\"epoch_3_loss\", 0.6239)\n",
        "    mlflow.log_metric(\"epoch_3_train_acc\", 72.64)\n",
        "    mlflow.log_metric(\"epoch_3_val_acc\", 84.73)\n",
        "    mlflow.log_metric(\"epoch_4_loss\", 0.7036)\n",
        "    mlflow.log_metric(\"epoch_4_train_acc\", 69.95)\n",
        "    mlflow.log_metric(\"epoch_4_val_acc\", 86.89)\n",
        "    mlflow.log_metric(\"epoch_5_loss\", 0.5512)\n",
        "    mlflow.log_metric(\"epoch_5_train_acc\", 75.38)\n",
        "    mlflow.log_metric(\"epoch_5_val_acc\", 86.49)\n",
        "    \n",
        "    mlflow.log_metric(\"summary_final_test_accuracy\", 84.74)\n",
        "    \n",
        "    print(\"✅ Custom ViT results logged!\")\n",
        "\n",
        "# ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n",
        "# 2️⃣ PRETRAINED VIT MODEL RESULTS\n",
        "# ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n",
        "\n",
        "print(\"\\n🚀 Logging PreTrained ViT Model Results...\")\n",
        "with mlflow.start_run(run_name=\"PreTrained-ViT-Model\"):\n",
        "    mlflow.log_param(\"model_type\", \"Vision Transformer (PreTrained ViT-B/16)\")\n",
        "    mlflow.log_param(\"epochs\", 5)\n",
        "    mlflow.log_param(\"optimizer\", \"AdamW\")\n",
        "    mlflow.log_param(\"learning_rate\", 1e-4)\n",
        "    mlflow.log_param(\"batch_size\", 32)\n",
        "    mlflow.log_param(\"pretrained_weights\", \"ViT_B_16_Weights.DEFAULT\")\n",
        "    \n",
        "    # PreTrained ViT Results\n",
        "    mlflow.log_metric(\"final_test_accuracy\", 97.12)\n",
        "    mlflow.log_metric(\"epoch_1_loss\", 0.5570)\n",
        "    mlflow.log_metric(\"epoch_1_train_acc\", 75.55)\n",
        "    mlflow.log_metric(\"epoch_1_val_acc\", 91.93)\n",
        "    mlflow.log_metric(\"epoch_2_loss\", 0.3417)\n",
        "    mlflow.log_metric(\"epoch_2_train_acc\", 85.24)\n",
        "    mlflow.log_metric(\"epoch_2_val_acc\", 96.56)\n",
        "    mlflow.log_metric(\"epoch_3_loss\", 0.3072)\n",
        "    mlflow.log_metric(\"epoch_3_train_acc\", 87.06)\n",
        "    mlflow.log_metric(\"epoch_3_val_acc\", 96.24)\n",
        "    mlflow.log_metric(\"epoch_4_loss\", 0.2820)\n",
        "    mlflow.log_metric(\"epoch_4_train_acc\", 87.87)\n",
        "    mlflow.log_metric(\"epoch_4_val_acc\", 96.64)\n",
        "    mlflow.log_metric(\"epoch_5_loss\", 0.2511)\n",
        "    mlflow.log_metric(\"epoch_5_train_acc\", 89.58)\n",
        "    mlflow.log_metric(\"epoch_5_val_acc\", 97.04)\n",
        "    \n",
        "    mlflow.log_metric(\"summary_final_test_accuracy\", 97.12)\n",
        "    \n",
        "    print(\"✅ PreTrained ViT results logged!\")\n",
        "\n",
        "# ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n",
        "# 3️⃣ PERFORMER MODEL RESULTS\n",
        "# ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n",
        "\n",
        "print(\"\\n🚀 Logging Performer Model Results...\")\n",
        "with mlflow.start_run(run_name=\"Performer-Model\"):\n",
        "    mlflow.log_param(\"model_type\", \"Performer (Efficient Transformer)\")\n",
        "    mlflow.log_param(\"epochs\", 5)\n",
        "    mlflow.log_param(\"optimizer\", \"AdamW\")\n",
        "    mlflow.log_param(\"learning_rate\", 1e-4)\n",
        "    mlflow.log_param(\"batch_size\", 32)\n",
        "    mlflow.log_param(\"num_layers\", 12)\n",
        "    mlflow.log_param(\"embed_dim\", 768)\n",
        "    mlflow.log_param(\"num_heads\", 12)\n",
        "    \n",
        "    # Performer Results\n",
        "    mlflow.log_metric(\"final_test_accuracy\", 88.90)\n",
        "    mlflow.log_metric(\"epoch_1_loss\", 1.4087)\n",
        "    mlflow.log_metric(\"epoch_1_train_acc\", 30.07)\n",
        "    mlflow.log_metric(\"epoch_1_val_acc\", 46.52)\n",
        "    mlflow.log_metric(\"epoch_2_loss\", 0.9530)\n",
        "    mlflow.log_metric(\"epoch_2_train_acc\", 57.67)\n",
        "    mlflow.log_metric(\"epoch_2_val_acc\", 77.30)\n",
        "    mlflow.log_metric(\"epoch_3_loss\", 0.6807)\n",
        "    mlflow.log_metric(\"epoch_3_train_acc\", 70.94)\n",
        "    mlflow.log_metric(\"epoch_3_val_acc\", 83.37)\n",
        "    mlflow.log_metric(\"epoch_4_loss\", 0.5925)\n",
        "    mlflow.log_metric(\"epoch_4_train_acc\", 74.37)\n",
        "    mlflow.log_metric(\"epoch_4_val_acc\", 87.77)\n",
        "    mlflow.log_metric(\"epoch_5_loss\", 0.5276)\n",
        "    mlflow.log_metric(\"epoch_5_train_acc\", 76.98)\n",
        "    mlflow.log_metric(\"epoch_5_val_acc\", 88.09)\n",
        "    \n",
        "    mlflow.log_metric(\"summary_final_test_accuracy\", 88.90)\n",
        "    \n",
        "    print(\"✅ Performer results logged!\")\n",
        "\n",
        "# ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n",
        "# SUMMARY\n",
        "# ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"✅ ALL RESULTS LOGGED TO MLFLOW!\")\n",
        "print(\"=\"*80)\n",
        "print(\"\\n📊 SUMMARY:\")\n",
        "print(f\"  • Custom ViT:        Final Test Accuracy = 84.74%\")\n",
        "print(f\"  • PreTrained ViT:    Final Test Accuracy = 97.12% ⭐ BEST\")\n",
        "print(f\"  • Performer Model:   Final Test Accuracy = 88.90%\")\n",
        "print(\"\\n🌐 Open MLflow Dashboard:\")\n",
        "print(\"   👉 http://localhost:5000\")\n",
        "print(\"\\n📁 Experiment: Blood-Cell-Classification-Results\")\n",
        "print(\"=\"*80 + \"\\n\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "mlflow",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.14.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
